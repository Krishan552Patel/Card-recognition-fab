{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training\n",
                "\n",
                "**Train on Colab ‚Üí Deploy on Jetson Nano**\n",
                "\n",
                "## Features:\n",
                "- MobileNetV3-Small backbone (60 FPS on Jetson)\n",
                "- Color histogram branch (distinguishes similar cards)\n",
                "- CosFace loss (stable for fine-grained recognition)\n",
                "- Sim-to-Real augmentation (train on scans, run on camera)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup - Clone from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== CONFIGURE YOUR GITHUB REPO =====\n",
                "GITHUB_REPO = \"YOUR_USERNAME/YOUR_REPO\"  # <-- CHANGE THIS!\n",
                "BRANCH = \"main\"\n",
                "\n",
                "# Clone repository\n",
                "import os\n",
                "\n",
                "WORK_DIR = \"/content/card_recognition\"\n",
                "if os.path.exists(WORK_DIR):\n",
                "    !rm -rf {WORK_DIR}\n",
                "\n",
                "!git clone https://github.com/{GITHUB_REPO}.git {WORK_DIR}\n",
                "os.chdir(WORK_DIR)\n",
                "print(f\"\\n‚úì Working directory: {os.getcwd()}\")\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q timm albumentations opencv-python-headless tqdm tensorboard imagehash"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Card Data from Google Drive\n",
                "\n",
                "**Your card data should be in Google Drive as:**\n",
                "```\n",
                "MyDrive/CardData/card_images.zip\n",
                "```\n",
                "\n",
                "Where `card_images.zip` contains your `D:\\SIAMESE DATASET\\LARGE SCALE OUTPUT` folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== CONFIGURE YOUR DATA PATH =====\n",
                "# Option 1: ZIP file (recommended for large datasets)\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "\n",
                "# Option 2: Folder (if already unzipped)\n",
                "FOLDER_PATH = \"/content/drive/MyDrive/CardData/LARGE SCALE OUTPUT\"\n",
                "\n",
                "# Local destination\n",
                "IMAGE_DIR = \"/content/card_recognition/Images\"\n",
                "\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "# Check which source exists\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    print(f\"Found ZIP: {ZIP_PATH}\")\n",
                "    print(\"Extracting... (this may take a few minutes)\")\n",
                "    \n",
                "    import zipfile\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "        zip_ref.extractall(\"/content/card_recognition/\")\n",
                "    \n",
                "    # Find the extracted folder\n",
                "    for item in os.listdir(\"/content/card_recognition/\"):\n",
                "        item_path = os.path.join(\"/content/card_recognition/\", item)\n",
                "        if os.path.isdir(item_path) and item not in ['.git', '__pycache__', 'model']:\n",
                "            # Check if it contains card folders\n",
                "            subfolders = [f for f in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, f))]\n",
                "            if len(subfolders) > 10:  # Likely card data\n",
                "                if item != \"Images\":\n",
                "                    os.rename(item_path, IMAGE_DIR)\n",
                "                print(f\"‚úì Extracted to: {IMAGE_DIR}\")\n",
                "                break\n",
                "\n",
                "elif os.path.exists(FOLDER_PATH):\n",
                "    print(f\"Found folder: {FOLDER_PATH}\")\n",
                "    print(\"Creating symlink... (fast, no copy)\")\n",
                "    \n",
                "    if os.path.exists(IMAGE_DIR):\n",
                "        os.remove(IMAGE_DIR)\n",
                "    os.symlink(FOLDER_PATH, IMAGE_DIR)\n",
                "    print(f\"‚úì Linked to: {IMAGE_DIR}\")\n",
                "\n",
                "else:\n",
                "    print(\"‚ùå Data not found!\")\n",
                "    print(f\"   Expected ZIP at: {ZIP_PATH}\")\n",
                "    print(f\"   Or folder at: {FOLDER_PATH}\")\n",
                "    print(\"\\nüì§ Please upload your card data to Google Drive.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify data\n",
                "if os.path.exists(IMAGE_DIR):\n",
                "    folders = [f for f in os.listdir(IMAGE_DIR) if os.path.isdir(os.path.join(IMAGE_DIR, f))]\n",
                "    print(f\"‚úì Found {len(folders):,} card folders\")\n",
                "    \n",
                "    # Count images\n",
                "    total = 0\n",
                "    for folder in folders[:100]:  # Sample first 100\n",
                "        folder_path = os.path.join(IMAGE_DIR, folder)\n",
                "        images = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg'))]\n",
                "        total += len(images)\n",
                "    \n",
                "    avg_per_card = total / min(100, len(folders))\n",
                "    print(f\"‚úì ~{avg_per_card:.1f} images per card\")\n",
                "    print(f\"‚úì Sample folders: {folders[:5]}\")\n",
                "else:\n",
                "    print(\"‚ùå Image directory not found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    \"\"\"Generalized Mean Pooling.\"\"\"\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.clamp(min=self.eps).pow(self.p)\n",
                "        x = F.adaptive_avg_pool2d(x, 1).pow(1.0 / self.p)\n",
                "        return x.view(x.size(0), -1)\n",
                "\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    \"\"\"Explicit color feature extraction for similar card distinction.\"\"\"\n",
                "    \n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(bins * 3, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, output_dim)\n",
                "        )\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = (x * self.std + self.mean) * 255\n",
                "        x_denorm = x_denorm.clamp(0, 255)\n",
                "        \n",
                "        batch_size = x.shape[0]\n",
                "        histograms = []\n",
                "        \n",
                "        for i in range(batch_size):\n",
                "            img = x_denorm[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            \n",
                "            h_hist = np.histogram(hsv[:,:,0], bins=self.bins, range=(0, 180))[0]\n",
                "            s_hist = np.histogram(hsv[:,:,1], bins=self.bins, range=(0, 256))[0]\n",
                "            v_hist = np.histogram(hsv[:,:,2], bins=self.bins, range=(0, 256))[0]\n",
                "            \n",
                "            hist = np.concatenate([h_hist, s_hist, v_hist]).astype(np.float32)\n",
                "            hist = hist / (hist.sum() + 1e-8)\n",
                "            histograms.append(hist)\n",
                "        \n",
                "        return self.fc(torch.tensor(np.stack(histograms), device=x.device, dtype=torch.float32))\n",
                "\n",
                "\n",
                "class CardEmbeddingNetV2(nn.Module):\n",
                "    \"\"\"Color-aware card embedding network.\"\"\"\n",
                "    \n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Visual backbone\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained,\n",
                "                                          num_classes=0, global_pool='')\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        \n",
                "        self.gem = GeM(p=3.0)\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        combined = torch.cat([visual, color], dim=1)\n",
                "        embedding = self.dropout(self.bn(self.fc(combined)))\n",
                "        return F.normalize(embedding, p=2, dim=1)\n",
                "\n",
                "\n",
                "# Test model\n",
                "model = CardEmbeddingNetV2()\n",
                "out = model(torch.randn(2, 3, 224, 224))\n",
                "print(f\"‚úì Model output: {out.shape}\")\n",
                "print(f\"‚úì Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ CosFace Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    \"\"\"CosFace: Additive Cosine Margin Loss - more stable than ArcFace.\"\"\"\n",
                "    \n",
                "    def __init__(self, num_classes, embedding_dim, scale=30.0, margin=0.35):\n",
                "        super().__init__()\n",
                "        self.scale = scale\n",
                "        self.margin = margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine)\n",
                "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        output = (cosine - one_hot * self.margin) * self.scale\n",
                "        return F.cross_entropy(output, labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset with Sim-to-Real Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "\n",
                "\n",
                "def get_train_transforms(size=224):\n",
                "    \"\"\"Sim-to-Real augmentations - simulate camera from perfect scans.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Affine(scale=(0.97, 1.03), rotate=(-2, 2), p=0.3),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=(3, 5)), A.MotionBlur(blur_limit=(3, 5))], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
                "        A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=10, val_shift_limit=10, p=0.2),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "class CardDataset(Dataset):\n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = Path(root_dir)\n",
                "        self.transform = transform\n",
                "        \n",
                "        self.card_folders = sorted([d for d in self.root_dir.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {f.name: i for i, f in enumerate(self.card_folders)}\n",
                "        \n",
                "        self.samples = []\n",
                "        for folder in self.card_folders:\n",
                "            for img in folder.glob(\"*\"):\n",
                "                if img.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
                "                    self.samples.append((img, self.class_to_idx[folder.name]))\n",
                "        \n",
                "        print(f\"Dataset: {len(self.card_folders)} cards, {len(self.samples)} images\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        path, label = self.samples[idx]\n",
                "        img = np.array(Image.open(path).convert('RGB'))\n",
                "        if self.transform:\n",
                "            img = self.transform(image=img)['image']\n",
                "        return img, label\n",
                "    \n",
                "    def get_num_classes(self): return len(self.card_folders)\n",
                "\n",
                "\n",
                "def create_dataloaders(root_dir, batch_size=64, num_workers=2):\n",
                "    train_ds = CardDataset(root_dir, get_train_transforms())\n",
                "    val_ds = CardDataset(root_dir, get_val_transforms())\n",
                "    \n",
                "    n = len(train_ds)\n",
                "    indices = np.random.permutation(n)\n",
                "    split = int(0.85 * n)\n",
                "    \n",
                "    train_loader = DataLoader(\n",
                "        torch.utils.data.Subset(train_ds, indices[:split]),\n",
                "        batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
                "        pin_memory=True, drop_last=True\n",
                "    )\n",
                "    val_loader = DataLoader(\n",
                "        torch.utils.data.Subset(val_ds, indices[split:]),\n",
                "        batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
                "    )\n",
                "    \n",
                "    return train_loader, val_loader, train_ds.get_num_classes()\n",
                "\n",
                "\n",
                "# Test\n",
                "if os.path.exists(IMAGE_DIR):\n",
                "    train_loader, val_loader, num_classes = create_dataloaders(IMAGE_DIR, batch_size=4)\n",
                "    print(f\"‚úì Classes: {num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'epochs': 100,\n",
                "    'batch_size': 64,\n",
                "    'learning_rate': 1e-3,\n",
                "    'weight_decay': 1e-4,\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 15,\n",
                "    'unfreeze_epoch': 6,  # Unfreeze backbone after this epoch\n",
                "}\n",
                "\n",
                "print(\"Configuration:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Train!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.cuda.amp import autocast, GradScaler\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# Directories\n",
                "CHECKPOINT_DIR = '/content/card_recognition/checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "# Data\n",
                "train_loader, val_loader, num_classes = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "\n",
                "# Model\n",
                "model = CardEmbeddingNetV2(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "# Freeze backbone initially\n",
                "for p in model.backbone.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
                "\n",
                "# Loss & Optimizer\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = GradScaler()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    # Unfreeze backbone\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(f\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters():\n",
                "            p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate'] / 10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    # Train\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        with autocast():\n",
                "            loss = criterion(model(images), labels)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    \n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validate\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    # Save best\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "            'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG\n",
                "        }, f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "        print(f\"  üíæ Saved best model\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(f\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best val loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Export & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history['train'], label='Train')\n",
                "plt.plot(history['val'], label='Val')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.title('Training Progress')\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
                "onnx_path = f\"{CHECKPOINT_DIR}/card_recognition.onnx\"\n",
                "\n",
                "torch.onnx.export(model, dummy, onnx_path,\n",
                "                  input_names=['image'], output_names=['embedding'],\n",
                "                  dynamic_axes={'image': {0: 'batch'}, 'embedding': {0: 'batch'}},\n",
                "                  opset_version=11)\n",
                "\n",
                "print(f\"‚úì ONNX: {onnx_path} ({os.path.getsize(onnx_path)/1024/1024:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Google Drive\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "\n",
                "import shutil\n",
                "for f in ['best_model.pth', 'card_recognition.onnx', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "\n",
                "print(f\"\\n‚úì Saved to: {DRIVE_OUTPUT}\")\n",
                "!ls -lh {DRIVE_OUTPUT}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Done!\n",
                "\n",
                "**Next steps:**\n",
                "1. Download from Google Drive: `CardRecognition_Models/`\n",
                "2. On Jetson Nano, convert to TensorRT:\n",
                "   ```bash\n",
                "   trtexec --onnx=card_recognition.onnx --saveEngine=card.engine --fp16\n",
                "   ```\n",
                "3. Run inference at ~50 FPS!"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}