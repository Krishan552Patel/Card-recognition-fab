{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training\n",
                "\n",
                "**Train on Colab ‚Üí Deploy on Jetson Nano**\n",
                "\n",
                "## Features:\n",
                "- MobileNetV3-Small backbone (60 FPS on Jetson)\n",
                "- Color histogram branch (distinguishes similar cards)\n",
                "- CosFace loss (stable for fine-grained recognition)\n",
                "- On-the-fly augmentation (rotation + sim-to-real)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup - Clone from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== YOUR GITHUB REPO =====\n",
                "GITHUB_REPO = \"Krishan552Patel/Card-recognition-fab\"\n",
                "\n",
                "import os\n",
                "\n",
                "# IMPORTANT: Reset to /content first to avoid directory errors\n",
                "os.chdir('/content')\n",
                "\n",
                "WORK_DIR = \"/content/card_recognition\"\n",
                "\n",
                "# Clean up if exists\n",
                "if os.path.exists(WORK_DIR):\n",
                "    !rm -rf {WORK_DIR}\n",
                "\n",
                "# Clone repository\n",
                "!git clone https://github.com/{GITHUB_REPO}.git {WORK_DIR}\n",
                "\n",
                "# Change to work directory\n",
                "os.chdir(WORK_DIR)\n",
                "print(f\"\\n‚úì Working directory: {os.getcwd()}\")\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q timm albumentations opencv-python-headless tqdm tensorboard imagehash"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Card Data from Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== CONFIGURE YOUR DATA PATH =====\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    print(f\"Found: {ZIP_PATH}\")\n",
                "    print(\"Extracting... (this may take a few minutes)\")\n",
                "    \n",
                "    if os.path.exists(IMAGE_DIR):\n",
                "        !rm -rf {IMAGE_DIR}\n",
                "    \n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "        zip_ref.extractall(IMAGE_DIR)\n",
                "    \n",
                "    images = [f for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
                "    print(f\"\\n‚úì Extracted {len(images):,} card images\")\n",
                "else:\n",
                "    print(f\"‚ùå ZIP not found: {ZIP_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate images and remove corrupted ones\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "print(\"Validating images (removing corrupted files)...\")\n",
                "\n",
                "image_dir = Path(IMAGE_DIR)\n",
                "all_images = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\")) + list(image_dir.glob(\"*.jpeg\"))\n",
                "\n",
                "valid_count = 0\n",
                "removed_count = 0\n",
                "\n",
                "for img_path in tqdm(all_images, desc=\"Checking\"):\n",
                "    try:\n",
                "        with Image.open(img_path) as img:\n",
                "            img.verify()  # Check if image is valid\n",
                "        # Also try to actually load it\n",
                "        with Image.open(img_path) as img:\n",
                "            img.load()\n",
                "        valid_count += 1\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è Removing corrupted: {img_path.name}\")\n",
                "        img_path.unlink()  # Delete corrupt file\n",
                "        removed_count += 1\n",
                "\n",
                "print(f\"\\n‚úì Valid images: {valid_count:,}\")\n",
                "if removed_count > 0:\n",
                "    print(f\"‚ö†Ô∏è Removed {removed_count} corrupted images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.clamp(min=self.eps).pow(self.p)\n",
                "        x = F.adaptive_avg_pool2d(x, 1).pow(1.0 / self.p)\n",
                "        return x.view(x.size(0), -1)\n",
                "\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(bins * 3, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, output_dim)\n",
                "        )\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = (x * self.std + self.mean) * 255\n",
                "        x_denorm = x_denorm.clamp(0, 255)\n",
                "        \n",
                "        batch_size = x.shape[0]\n",
                "        histograms = []\n",
                "        \n",
                "        for i in range(batch_size):\n",
                "            img = x_denorm[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            \n",
                "            h_hist = np.histogram(hsv[:,:,0], bins=self.bins, range=(0, 180))[0]\n",
                "            s_hist = np.histogram(hsv[:,:,1], bins=self.bins, range=(0, 256))[0]\n",
                "            v_hist = np.histogram(hsv[:,:,2], bins=self.bins, range=(0, 256))[0]\n",
                "            \n",
                "            hist = np.concatenate([h_hist, s_hist, v_hist]).astype(np.float32)\n",
                "            hist = hist / (hist.sum() + 1e-8)\n",
                "            histograms.append(hist)\n",
                "        \n",
                "        return self.fc(torch.tensor(np.stack(histograms), device=x.device, dtype=torch.float32))\n",
                "\n",
                "\n",
                "class CardEmbeddingNetV2(nn.Module):\n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained,\n",
                "                                          num_classes=0, global_pool='')\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        \n",
                "        self.gem = GeM(p=3.0)\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        combined = torch.cat([visual, color], dim=1)\n",
                "        embedding = self.dropout(self.bn(self.fc(combined)))\n",
                "        return F.normalize(embedding, p=2, dim=1)\n",
                "\n",
                "\n",
                "model = CardEmbeddingNetV2()\n",
                "out = model(torch.randn(2, 3, 224, 224))\n",
                "print(f\"‚úì Model output: {out.shape}, Params: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ CosFace Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, embedding_dim, scale=30.0, margin=0.35):\n",
                "        super().__init__()\n",
                "        self.scale = scale\n",
                "        self.margin = margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine)\n",
                "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        output = (cosine - one_hot * self.margin) * self.scale\n",
                "        return F.cross_entropy(output, labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset with Error Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "import random\n",
                "\n",
                "\n",
                "def get_train_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Affine(scale=(0.97, 1.03), rotate=(-2, 2), p=0.3),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=(3, 5)), A.MotionBlur(blur_limit=(3, 5))], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
                "        A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=10, val_shift_limit=10, p=0.2),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "class CardDatasetWithRotation(Dataset):\n",
                "    \"\"\"Dataset with robust error handling for corrupted images.\"\"\"\n",
                "    \n",
                "    def __init__(self, image_dir, transform=None, rotations=[0, 90, 180, 270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.rotations = rotations\n",
                "        \n",
                "        self.images = sorted([\n",
                "            f for f in self.image_dir.iterdir()\n",
                "            if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']\n",
                "        ])\n",
                "        \n",
                "        self.num_cards = len(self.images)\n",
                "        self.filename_to_idx = {img.stem: idx for idx, img in enumerate(self.images)}\n",
                "        self.idx_to_filename = {idx: img.stem for idx, img in enumerate(self.images)}\n",
                "        \n",
                "        self.samples = []\n",
                "        for img_idx, img_path in enumerate(self.images):\n",
                "            for rot in self.rotations:\n",
                "                self.samples.append((img_idx, rot))\n",
                "        \n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(self.rotations)} rotations = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        img_path = self.images[img_idx]\n",
                "        \n",
                "        try:\n",
                "            # Load image with error handling\n",
                "            with Image.open(img_path) as pil_img:\n",
                "                img = np.array(pil_img.convert('RGB'))\n",
                "            \n",
                "            # Apply rotation\n",
                "            if rotation == 90:\n",
                "                img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rotation == 180:\n",
                "                img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rotation == 270:\n",
                "                img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            \n",
                "            if self.transform:\n",
                "                img = self.transform(image=img)['image']\n",
                "            \n",
                "            return img, img_idx\n",
                "            \n",
                "        except Exception as e:\n",
                "            # Return a random valid sample if this one fails\n",
                "            print(f\"\\n‚ö†Ô∏è Error loading {img_path.name}: {e}\")\n",
                "            new_idx = random.randint(0, len(self.samples) - 1)\n",
                "            return self.__getitem__(new_idx)\n",
                "    \n",
                "    def get_num_classes(self):\n",
                "        return self.num_cards\n",
                "\n",
                "\n",
                "def create_dataloaders(image_dir, batch_size=64, num_workers=2, val_split=0.15):\n",
                "    train_ds = CardDatasetWithRotation(image_dir, get_train_transforms())\n",
                "    val_ds = CardDatasetWithRotation(image_dir, get_val_transforms(), rotations=[0])\n",
                "    \n",
                "    n_cards = train_ds.num_cards\n",
                "    indices = np.random.permutation(n_cards)\n",
                "    split = int((1 - val_split) * n_cards)\n",
                "    \n",
                "    train_card_indices = set(indices[:split])\n",
                "    val_card_indices = set(indices[split:])\n",
                "    \n",
                "    train_sample_indices = [i for i, (card_idx, _) in enumerate(train_ds.samples) if card_idx in train_card_indices]\n",
                "    val_sample_indices = [i for i, (card_idx, _) in enumerate(val_ds.samples) if card_idx in val_card_indices]\n",
                "    \n",
                "    # Use num_workers=0 to avoid multiprocessing issues with error handling\n",
                "    train_loader = DataLoader(\n",
                "        torch.utils.data.Subset(train_ds, train_sample_indices),\n",
                "        batch_size=batch_size, shuffle=True, num_workers=0,\n",
                "        pin_memory=True, drop_last=True\n",
                "    )\n",
                "    val_loader = DataLoader(\n",
                "        torch.utils.data.Subset(val_ds, val_sample_indices),\n",
                "        batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
                "    )\n",
                "    \n",
                "    print(f\"Train: {len(train_sample_indices)} samples ({len(train_card_indices)} cards)\")\n",
                "    print(f\"Val: {len(val_sample_indices)} samples ({len(val_card_indices)} cards)\")\n",
                "    \n",
                "    return train_loader, val_loader, train_ds.get_num_classes(), train_ds\n",
                "\n",
                "\n",
                "if os.path.exists(IMAGE_DIR):\n",
                "    train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, batch_size=4)\n",
                "    print(f\"‚úì Classes: {num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'epochs': 100,\n",
                "    'batch_size': 64,\n",
                "    'learning_rate': 1e-3,\n",
                "    'weight_decay': 1e-4,\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 15,\n",
                "    'unfreeze_epoch': 6,\n",
                "}\n",
                "\n",
                "print(\"Configuration:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Train!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "\n",
                "model = CardEmbeddingNetV2(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "for p in model.backbone.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "\n",
                "# Use new autocast syntax\n",
                "scaler = torch.cuda.amp.GradScaler()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(f\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters():\n",
                "            p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate'] / 10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        # Updated autocast syntax\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    \n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "            'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG\n",
                "        }, f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "        print(f\"  üíæ Saved best model\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(f\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best val loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Export & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history['train'], label='Train')\n",
                "plt.plot(history['val'], label='Val')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.title('Training Progress')\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
                "onnx_path = f\"{CHECKPOINT_DIR}/card_recognition.onnx\"\n",
                "\n",
                "torch.onnx.export(model, dummy, onnx_path,\n",
                "                  input_names=['image'], output_names=['embedding'],\n",
                "                  dynamic_axes={'image': {0: 'batch'}, 'embedding': {0: 'batch'}},\n",
                "                  opset_version=11)\n",
                "\n",
                "print(f\"‚úì ONNX: {onnx_path} ({os.path.getsize(onnx_path)/1024/1024:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Google Drive\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "\n",
                "import shutil\n",
                "for f in ['best_model.pth', 'card_recognition.onnx', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "\n",
                "print(f\"\\n‚úì Saved to: {DRIVE_OUTPUT}\")\n",
                "!ls -lh {DRIVE_OUTPUT}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Test the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building reference embeddings...\")\n",
                "\n",
                "model.eval()\n",
                "reference_embeddings = []\n",
                "reference_names = []\n",
                "test_transform = get_val_transforms()\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            img_tensor = test_transform(image=img)['image'].unsqueeze(0).to(device)\n",
                "            embedding = model(img_tensor)\n",
                "            reference_embeddings.append(embedding.cpu())\n",
                "            reference_names.append(img_path.stem)\n",
                "        except:\n",
                "            pass\n",
                "\n",
                "reference_embeddings = torch.cat(reference_embeddings, dim=0)\n",
                "print(f\"‚úì Built {len(reference_embeddings)} reference embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def identify_card(image_path, top_k=5):\n",
                "    img = np.array(Image.open(image_path).convert('RGB'))\n",
                "    img_tensor = test_transform(image=img)['image'].unsqueeze(0).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        query_embedding = model(img_tensor).cpu()\n",
                "    \n",
                "    similarities = F.cosine_similarity(query_embedding, reference_embeddings)\n",
                "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
                "    \n",
                "    return [{'name': reference_names[idx], 'similarity': similarities[idx].item() * 100} for idx in top_indices]\n",
                "\n",
                "print(\"‚úì Identification function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on random cards\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "test_cards = random.sample(list(train_ds.images), min(5, len(train_ds.images)))\n",
                "\n",
                "fig, axes = plt.subplots(1, len(test_cards), figsize=(4*len(test_cards), 5))\n",
                "if len(test_cards) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"IDENTIFICATION RESULTS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "correct = 0\n",
                "for i, card_path in enumerate(test_cards):\n",
                "    results = identify_card(card_path)\n",
                "    actual_name = card_path.stem\n",
                "    is_correct = results[0]['name'] == actual_name\n",
                "    if is_correct: correct += 1\n",
                "    \n",
                "    img = Image.open(card_path)\n",
                "    axes[i].imshow(img)\n",
                "    axes[i].axis('off')\n",
                "    axes[i].set_title(f\"{'‚úÖ' if is_correct else '‚ùå'} {results[0]['similarity']:.1f}%\", fontsize=12)\n",
                "    \n",
                "    print(f\"\\nCard: {actual_name}\")\n",
                "    print(f\"Status: {'CORRECT ‚úÖ' if is_correct else 'WRONG ‚ùå'}\")\n",
                "    for j, r in enumerate(results):\n",
                "        print(f\"  {'‚Üí' if r['name'] == actual_name else ' '} {j+1}. {r['name']} ({r['similarity']:.1f}%)\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/test_results.png\")\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n\" + \"=\"*60)\n",
                "print(f\"Accuracy: {correct}/{len(test_cards)} = {100*correct/len(test_cards):.1f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**Files saved to Google Drive:**\n",
                "- `best_model.pth` - PyTorch checkpoint\n",
                "- `card_recognition.onnx` - For Jetson Nano\n",
                "- `training.png` - Training curves\n",
                "\n",
                "**Deploy to Jetson Nano:**\n",
                "```bash\n",
                "trtexec --onnx=card_recognition.onnx --saveEngine=card.engine --fp16\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}