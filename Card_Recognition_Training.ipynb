{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training\n",
                "\n",
                "**Train on Colab ‚Üí Deploy on Jetson Nano**\n",
                "\n",
                "Features: MobileNetV3-Small, Color histogram, CosFace loss, On-the-fly augmentation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone from GitHub (skips if already done)\n",
                "GITHUB_REPO = \"Krishan552Patel/Card-recognition-fab\"\n",
                "\n",
                "import os\n",
                "os.chdir('/content')\n",
                "\n",
                "WORK_DIR = \"/content/card_recognition\"\n",
                "\n",
                "if os.path.exists(WORK_DIR) and os.path.exists(f\"{WORK_DIR}/.git\"):\n",
                "    print(\"‚úì Repo already cloned, pulling latest...\")\n",
                "    os.chdir(WORK_DIR)\n",
                "    !git pull\n",
                "else:\n",
                "    if os.path.exists(WORK_DIR):\n",
                "        !rm -rf {WORK_DIR}\n",
                "    !git clone https://github.com/{GITHUB_REPO}.git {WORK_DIR}\n",
                "    os.chdir(WORK_DIR)\n",
                "    print(\"‚úì Cloned successfully\")\n",
                "\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (skips if already installed)\n",
                "try:\n",
                "    import timm, albumentations\n",
                "    print(\"‚úì Dependencies already installed\")\n",
                "except ImportError:\n",
                "    print(\"Installing dependencies...\")\n",
                "    !pip install -q timm albumentations opencv-python-headless tqdm tensorboard imagehash\n",
                "    print(\"‚úì Installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Data from Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract ZIP (skips if already done)\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "EXTRACTED_MARKER = f\"{IMAGE_DIR}/.extracted\"\n",
                "\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "if os.path.exists(EXTRACTED_MARKER):\n",
                "    images = list(Path(IMAGE_DIR).glob(\"*.jpg\")) + list(Path(IMAGE_DIR).glob(\"*.png\"))\n",
                "    print(f\"‚úì Already extracted: {len(images):,} images\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(f\"Extracting {ZIP_PATH}...\")\n",
                "    if os.path.exists(IMAGE_DIR):\n",
                "        !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    \n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "        zip_ref.extractall(IMAGE_DIR)\n",
                "    \n",
                "    # Create marker file\n",
                "    Path(EXTRACTED_MARKER).touch()\n",
                "    \n",
                "    images = list(Path(IMAGE_DIR).glob(\"*.jpg\")) + list(Path(IMAGE_DIR).glob(\"*.png\"))\n",
                "    print(f\"‚úì Extracted {len(images):,} images\")\n",
                "else:\n",
                "    print(f\"‚ùå ZIP not found: {ZIP_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate images and save corrupted list (skips if already done)\n",
                "from PIL import Image\n",
                "from tqdm.notebook import tqdm\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "VALIDATED_MARKER = f\"{IMAGE_DIR}/.validated\"\n",
                "CORRUPTED_LOG = \"/content/drive/MyDrive/CardData/corrupted_images.json\"\n",
                "\n",
                "if os.path.exists(VALIDATED_MARKER):\n",
                "    print(\"‚úì Images already validated\")\n",
                "    if os.path.exists(CORRUPTED_LOG):\n",
                "        with open(CORRUPTED_LOG, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        print(f\"  Previous corrupted files: {len(data.get('corrupted', []))}\")\n",
                "else:\n",
                "    print(\"Validating images...\")\n",
                "    image_dir = Path(IMAGE_DIR)\n",
                "    all_images = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\")) + list(image_dir.glob(\"*.jpeg\"))\n",
                "    \n",
                "    valid_count = 0\n",
                "    corrupted_files = []\n",
                "    \n",
                "    for img_path in tqdm(all_images, desc=\"Checking\"):\n",
                "        try:\n",
                "            with Image.open(img_path) as img:\n",
                "                img.verify()\n",
                "            with Image.open(img_path) as img:\n",
                "                img.load()\n",
                "            valid_count += 1\n",
                "        except Exception as e:\n",
                "            corrupted_files.append({\n",
                "                'filename': img_path.name,\n",
                "                'error': str(e)\n",
                "            })\n",
                "            print(f\"  ‚ö†Ô∏è Corrupted: {img_path.name}\")\n",
                "            img_path.unlink()  # Remove corrupted file\n",
                "    \n",
                "    # Save corrupted list to Google Drive\n",
                "    log_data = {\n",
                "        'validated_at': datetime.now().isoformat(),\n",
                "        'total_checked': len(all_images),\n",
                "        'valid_count': valid_count,\n",
                "        'corrupted': corrupted_files\n",
                "    }\n",
                "    \n",
                "    with open(CORRUPTED_LOG, 'w') as f:\n",
                "        json.dump(log_data, f, indent=2)\n",
                "    \n",
                "    # Create marker\n",
                "    Path(VALIDATED_MARKER).touch()\n",
                "    \n",
                "    print(f\"\\n‚úì Valid: {valid_count:,}\")\n",
                "    if corrupted_files:\n",
                "        print(f\"‚ö†Ô∏è Corrupted: {len(corrupted_files)} (list saved to {CORRUPTED_LOG})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.clamp(min=self.eps).pow(self.p)\n",
                "        x = F.adaptive_avg_pool2d(x, 1).pow(1.0 / self.p)\n",
                "        return x.view(x.size(0), -1)\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(bins * 3, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, output_dim)\n",
                "        )\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = ((x * self.std + self.mean) * 255).clamp(0, 255)\n",
                "        histograms = []\n",
                "        for i in range(x.shape[0]):\n",
                "            img = x_denorm[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            h = np.histogram(hsv[:,:,0], bins=self.bins, range=(0, 180))[0]\n",
                "            s = np.histogram(hsv[:,:,1], bins=self.bins, range=(0, 256))[0]\n",
                "            v = np.histogram(hsv[:,:,2], bins=self.bins, range=(0, 256))[0]\n",
                "            hist = np.concatenate([h, s, v]).astype(np.float32)\n",
                "            histograms.append(hist / (hist.sum() + 1e-8))\n",
                "        return self.fc(torch.tensor(np.stack(histograms), device=x.device, dtype=torch.float32))\n",
                "\n",
                "class CardEmbeddingNetV2(nn.Module):\n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained,\n",
                "                                          num_classes=0, global_pool='')\n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        self.gem = GeM(p=3.0)\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        embedding = self.dropout(self.bn(self.fc(torch.cat([visual, color], dim=1))))\n",
                "        return F.normalize(embedding, p=2, dim=1)\n",
                "\n",
                "model = CardEmbeddingNetV2()\n",
                "print(f\"‚úì Model: {sum(p.numel() for p in model.parameters()):,} params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Loss & Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, embedding_dim, scale=30.0, margin=0.35):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        return F.cross_entropy((cosine - one_hot * self.margin) * self.scale, labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "import random\n",
                "\n",
                "def get_train_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Affine(scale=(0.97, 1.03), rotate=(-2, 2), p=0.3),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=(3,5)), A.MotionBlur(blur_limit=(3,5))], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
                "        A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=10, val_shift_limit=10, p=0.2),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "class CardDatasetWithRotation(Dataset):\n",
                "    def __init__(self, image_dir, transform=None, rotations=[0, 90, 180, 270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.rotations = rotations\n",
                "        self.images = sorted([f for f in self.image_dir.iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']])\n",
                "        self.num_cards = len(self.images)\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        try:\n",
                "            with Image.open(self.images[img_idx]) as pil_img:\n",
                "                img = np.array(pil_img.convert('RGB'))\n",
                "            if rotation == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rotation == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rotation == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            if self.transform: img = self.transform(image=img)['image']\n",
                "            return img, img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples)-1))\n",
                "    \n",
                "    def get_num_classes(self): return self.num_cards\n",
                "\n",
                "def create_dataloaders(image_dir, batch_size=64, val_split=0.15):\n",
                "    train_ds = CardDatasetWithRotation(image_dir, get_train_transforms())\n",
                "    val_ds = CardDatasetWithRotation(image_dir, get_val_transforms(), rotations=[0])\n",
                "    indices = np.random.permutation(train_ds.num_cards)\n",
                "    split = int((1 - val_split) * train_ds.num_cards)\n",
                "    train_idx = set(indices[:split])\n",
                "    val_idx = set(indices[split:])\n",
                "    train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "    val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "    train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                              batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
                "    val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                            batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
                "    print(f\"Train: {len(train_samples)} | Val: {len(val_samples)}\")\n",
                "    return train_loader, val_loader, train_ds.get_num_classes(), train_ds\n",
                "\n",
                "train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, batch_size=4)\n",
                "print(f\"‚úì Classes: {num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'epochs': 100, 'batch_size': 64, 'learning_rate': 1e-3,\n",
                "    'weight_decay': 1e-4, 'embedding_dim': 512, 'patience': 15, 'unfreeze_epoch': 6\n",
                "}\n",
                "print(\"Config:\", CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "# Check if we can resume\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "start_epoch = 1\n",
                "\n",
                "train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "model = CardEmbeddingNetV2(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "if os.path.exists(RESUME_PATH):\n",
                "    print(f\"Found existing checkpoint, loading...\")\n",
                "    ckpt = torch.load(RESUME_PATH)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    start_epoch = ckpt.get('epoch', 0) + 1\n",
                "    print(f\"‚úì Resuming from epoch {start_epoch}\")\n",
                "else:\n",
                "    for p in model.backbone.parameters():\n",
                "        p.requires_grad = False\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.cuda.amp.GradScaler()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop (can resume)\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "for epoch in range(start_epoch, CONFIG['epochs'] + 1):\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(f\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters():\n",
                "            p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate'] / 10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "            'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG\n",
                "        }, RESUME_PATH)\n",
                "        print(f\"  üíæ Saved\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(f\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Export & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if history['train']:\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    plt.plot(history['train'], label='Train')\n",
                "    plt.plot(history['val'], label='Val')\n",
                "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
                "    plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "onnx_path = f\"{CHECKPOINT_DIR}/card_recognition.onnx\"\n",
                "\n",
                "if not os.path.exists(onnx_path):\n",
                "    ckpt = torch.load(RESUME_PATH)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    model.eval()\n",
                "    dummy = torch.randn(1, 3, 224, 224).to(device)\n",
                "    torch.onnx.export(model, dummy, onnx_path,\n",
                "                      input_names=['image'], output_names=['embedding'],\n",
                "                      dynamic_axes={'image': {0: 'batch'}, 'embedding': {0: 'batch'}},\n",
                "                      opset_version=11)\n",
                "    print(f\"‚úì ONNX exported: {os.path.getsize(onnx_path)/1024/1024:.1f} MB\")\n",
                "else:\n",
                "    print(\"‚úì ONNX already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Google Drive\n",
                "import shutil\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "\n",
                "for f in ['best_model.pth', 'card_recognition.onnx', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "\n",
                "print(f\"‚úì Saved to: {DRIVE_OUTPUT}\")\n",
                "!ls -lh {DRIVE_OUTPUT}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Test Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build reference embeddings\n",
                "print(\"Building reference embeddings...\")\n",
                "model.eval()\n",
                "test_transform = get_val_transforms()\n",
                "reference_embeddings, reference_names = [], []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            img_tensor = test_transform(image=img)['image'].unsqueeze(0).to(device)\n",
                "            reference_embeddings.append(model(img_tensor).cpu())\n",
                "            reference_names.append(img_path.stem)\n",
                "        except: pass\n",
                "\n",
                "reference_embeddings = torch.cat(reference_embeddings, dim=0)\n",
                "print(f\"‚úì {len(reference_embeddings)} embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on random cards\n",
                "test_cards = random.sample(list(train_ds.images), min(5, len(train_ds.images)))\n",
                "fig, axes = plt.subplots(1, len(test_cards), figsize=(4*len(test_cards), 5))\n",
                "if len(test_cards) == 1: axes = [axes]\n",
                "\n",
                "correct = 0\n",
                "for i, card_path in enumerate(test_cards):\n",
                "    img = np.array(Image.open(card_path).convert('RGB'))\n",
                "    with torch.no_grad():\n",
                "        query = model(test_transform(image=img)['image'].unsqueeze(0).to(device)).cpu()\n",
                "    sims = F.cosine_similarity(query, reference_embeddings)\n",
                "    top_idx = sims.argmax().item()\n",
                "    \n",
                "    actual = card_path.stem\n",
                "    predicted = reference_names[top_idx]\n",
                "    is_correct = actual == predicted\n",
                "    if is_correct: correct += 1\n",
                "    \n",
                "    axes[i].imshow(Image.open(card_path))\n",
                "    axes[i].axis('off')\n",
                "    axes[i].set_title(f\"{'‚úÖ' if is_correct else '‚ùå'} {sims[top_idx].item()*100:.1f}%\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/test_results.png\")\n",
                "plt.show()\n",
                "print(f\"Accuracy: {correct}/{len(test_cards)} = {100*correct/len(test_cards):.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**Corrupted images list saved to:** `MyDrive/CardData/corrupted_images.json`\n",
                "\n",
                "**Models saved to:** `MyDrive/CardRecognition_Models/`\n",
                "\n",
                "**Deploy to Jetson:**\n",
                "```bash\n",
                "trtexec --onnx=card_recognition.onnx --saveEngine=card.engine --fp16\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}