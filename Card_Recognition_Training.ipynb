{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training V2\n",
                "\n",
                "**Train on Colab ‚Üí Deploy on Jetson Nano**\n",
                "\n",
                "Optimized for 14K+ unique cards with improved hyperparameters\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & GPU Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU - Use A100 for best performance!\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    print(f\"GPU: {gpu_name}\")\n",
                "    if 'A100' in gpu_name:\n",
                "        print(\"‚úÖ A100 detected - optimal performance!\")\n",
                "    elif 'V100' in gpu_name:\n",
                "        print(\"‚úÖ V100 detected - good performance\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è Consider switching to A100 in Runtime > Change runtime type\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q timm albumentations opencv-python-headless tqdm tensorboard imagehash"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Mount Google Drive & Extract Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "\n",
                "# Extract if needed\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.extracted\"):\n",
                "    images = list(Path(IMAGE_DIR).glob(\"*.[jp][pn][g]*\"))\n",
                "    print(f\"‚úì Already extracted: {len(images):,} images\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(f\"Extracting {ZIP_PATH}...\")\n",
                "    !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
                "        z.extractall(IMAGE_DIR)\n",
                "    Path(f\"{IMAGE_DIR}/.extracted\").touch()\n",
                "    images = list(Path(IMAGE_DIR).glob(\"*.[jp][pn][g]*\"))\n",
                "    print(f\"‚úì Extracted {len(images):,} images\")\n",
                "else:\n",
                "    print(f\"‚ùå ZIP not found: {ZIP_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate images (remove corrupted ones)\n",
                "from PIL import Image\n",
                "from tqdm.notebook import tqdm\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.validated\"):\n",
                "    print(\"‚úì Images already validated\")\n",
                "else:\n",
                "    print(\"Validating images...\")\n",
                "    corrupted = []\n",
                "    for img_path in tqdm(list(Path(IMAGE_DIR).glob(\"*\"))):\n",
                "        if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:\n",
                "            try:\n",
                "                with Image.open(img_path) as img:\n",
                "                    img.verify()\n",
                "                with Image.open(img_path) as img:\n",
                "                    img.load()\n",
                "            except:\n",
                "                corrupted.append(img_path.name)\n",
                "                img_path.unlink()\n",
                "    \n",
                "    # Save corrupted list\n",
                "    with open('/content/drive/MyDrive/CardData/corrupted_images.json', 'w') as f:\n",
                "        json.dump({'date': datetime.now().isoformat(), 'files': corrupted}, f)\n",
                "    \n",
                "    Path(f\"{IMAGE_DIR}/.validated\").touch()\n",
                "    print(f\"‚úì Removed {len(corrupted)} corrupted images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.clamp(min=self.eps).pow(self.p)\n",
                "        x = F.adaptive_avg_pool2d(x, 1).pow(1.0 / self.p)\n",
                "        return x.view(x.size(0), -1)\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(bins * 3, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, output_dim)\n",
                "        )\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = ((x * self.std + self.mean) * 255).clamp(0, 255)\n",
                "        histograms = []\n",
                "        for i in range(x.shape[0]):\n",
                "            img = x_denorm[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            h = np.histogram(hsv[:,:,0], bins=self.bins, range=(0, 180))[0]\n",
                "            s = np.histogram(hsv[:,:,1], bins=self.bins, range=(0, 256))[0]\n",
                "            v = np.histogram(hsv[:,:,2], bins=self.bins, range=(0, 256))[0]\n",
                "            hist = np.concatenate([h, s, v]).astype(np.float32)\n",
                "            histograms.append(hist / (hist.sum() + 1e-8))\n",
                "        return self.fc(torch.tensor(np.stack(histograms), device=x.device, dtype=torch.float32))\n",
                "\n",
                "class CardEmbeddingNetV2(nn.Module):\n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained,\n",
                "                                          num_classes=0, global_pool='')\n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        self.gem = GeM(p=3.0)\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        embedding = self.dropout(self.bn(self.fc(torch.cat([visual, color], dim=1))))\n",
                "        return F.normalize(embedding, p=2, dim=1)\n",
                "\n",
                "model = CardEmbeddingNetV2()\n",
                "print(f\"‚úì Model: {sum(p.numel() for p in model.parameters()):,} params\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Loss & Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPROVED: Higher margin and scale for better separation\n",
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, embedding_dim, scale=64.0, margin=0.5):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        return F.cross_entropy((cosine - one_hot * self.margin) * self.scale, labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import random\n",
                "\n",
                "def get_train_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Affine(scale=(0.97, 1.03), rotate=(-2, 2), p=0.3),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=(3,5)), A.MotionBlur(blur_limit=(3,5))], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
                "        A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=10, val_shift_limit=10, p=0.2),\n",
                "        A.GaussianBlur(blur_limit=(3, 5), p=0.1),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "class CardDatasetWithRotation(Dataset):\n",
                "    def __init__(self, image_dir, transform=None, rotations=[0, 90, 180, 270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.rotations = rotations\n",
                "        self.images = sorted([f for f in self.image_dir.iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']])\n",
                "        self.num_cards = len(self.images)\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        try:\n",
                "            with Image.open(self.images[img_idx]) as pil_img:\n",
                "                img = np.array(pil_img.convert('RGB'))\n",
                "            if rotation == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rotation == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rotation == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            if self.transform: img = self.transform(image=img)['image']\n",
                "            return img, img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples)-1))\n",
                "    \n",
                "    def get_num_classes(self): return self.num_cards\n",
                "\n",
                "print(\"‚úì Dataset classes ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Training Configuration (IMPROVED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPROVED CONFIG - prevents embedding collapse\n",
                "CONFIG = {\n",
                "    'epochs': 100,\n",
                "    'batch_size': 128,         # Larger batch for A100\n",
                "    'learning_rate': 3e-4,     # Lower LR - more stable\n",
                "    'weight_decay': 5e-4,      # More regularization\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 10,            # Stop earlier if overfitting\n",
                "    'unfreeze_epoch': 20,      # Keep backbone frozen longer\n",
                "    'margin': 0.5,             # Higher margin for separation\n",
                "    'scale': 64.0,             # Higher scale for gradients\n",
                "}\n",
                "\n",
                "print(\"‚úì Improved Config:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataloaders\n",
                "def create_dataloaders(image_dir, batch_size=64, val_split=0.15):\n",
                "    train_ds = CardDatasetWithRotation(image_dir, get_train_transforms())\n",
                "    val_ds = CardDatasetWithRotation(image_dir, get_val_transforms(), rotations=[0])\n",
                "    \n",
                "    indices = np.random.permutation(train_ds.num_cards)\n",
                "    split = int((1 - val_split) * train_ds.num_cards)\n",
                "    train_idx = set(indices[:split])\n",
                "    val_idx = set(indices[split:])\n",
                "    \n",
                "    train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "    val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "    \n",
                "    train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                              batch_size=batch_size, shuffle=True, num_workers=2, \n",
                "                              pin_memory=True, drop_last=True)\n",
                "    val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                            batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "    \n",
                "    print(f\"Train: {len(train_samples):,} | Val: {len(val_samples):,}\")\n",
                "    return train_loader, val_loader, train_ds.get_num_classes(), train_ds\n",
                "\n",
                "train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "print(f\"‚úì Classes: {num_classes:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model and training\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "model = CardEmbeddingNetV2(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "# Freeze backbone initially\n",
                "for p in model.backbone.parameters():\n",
                "    p.requires_grad = False\n",
                "print(\"‚úì Backbone frozen\")\n",
                "\n",
                "# Loss with improved margin\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim'], \n",
                "                        scale=CONFIG['scale'], margin=CONFIG['margin']).to(device)\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "print(\"‚úì Ready to train\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop with improved settings\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    # Unfreeze backbone at specified epoch\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(f\"\\nüîì Unfreezing backbone at epoch {epoch}...\")\n",
                "        for p in model.backbone.parameters():\n",
                "            p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate'] / 10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs']-epoch)\n",
                "    \n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "            'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG\n",
                "        }, RESUME_PATH)\n",
                "        print(f\"  üíæ Saved best model\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch}!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best val loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot training curves\n",
                "if history['train']:\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    plt.plot(history['train'], label='Train')\n",
                "    plt.plot(history['val'], label='Val')\n",
                "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
                "    plt.title('Training Progress')\n",
                "    plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Google Drive\n",
                "import shutil\n",
                "for f in ['best_model.pth', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "        print(f\"‚úì Saved {f}\")\n",
                "\n",
                "print(f\"\\n‚úì All files saved to: {DRIVE_OUTPUT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Test Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build reference embeddings\n",
                "print(\"Building reference embeddings...\")\n",
                "\n",
                "# Reload best model\n",
                "ckpt = torch.load(RESUME_PATH)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "test_transform = get_val_transforms()\n",
                "reference_embeddings = []\n",
                "reference_names = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            img_tensor = test_transform(image=img)['image'].unsqueeze(0).to(device)\n",
                "            emb = model(img_tensor)\n",
                "            reference_embeddings.append(emb.cpu())\n",
                "            reference_names.append(img_path.stem)\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {img_path.name}\")\n",
                "\n",
                "reference_embeddings = torch.cat(reference_embeddings, dim=0)\n",
                "print(f\"‚úì {len(reference_embeddings):,} embeddings built\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on random cards with detailed output\n",
                "test_cards = random.sample(list(train_ds.images), min(10, len(train_ds.images)))\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"IDENTIFICATION RESULTS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "correct = 0\n",
                "for i, card_path in enumerate(test_cards):\n",
                "    img = np.array(Image.open(card_path).convert('RGB'))\n",
                "    with torch.no_grad():\n",
                "        query = model(test_transform(image=img)['image'].unsqueeze(0).to(device)).cpu()\n",
                "    sims = F.cosine_similarity(query, reference_embeddings)\n",
                "    \n",
                "    top_indices = sims.argsort(descending=True)[:5]\n",
                "    actual = card_path.stem\n",
                "    predicted = reference_names[top_indices[0]]\n",
                "    is_correct = actual == predicted\n",
                "    if is_correct: correct += 1\n",
                "    \n",
                "    status = \"CORRECT\" if is_correct else \"WRONG\"\n",
                "    print(f\"\\n[{status}] Card #{i+1}\")\n",
                "    print(f\"   Actual:    {actual}\")\n",
                "    print(f\"   Predicted: {predicted} ({sims[top_indices[0]].item()*100:.1f}%)\")\n",
                "    print(f\"   Top 5:\")\n",
                "    for rank, idx in enumerate(top_indices):\n",
                "        marker = \"->\" if reference_names[idx] == actual else \"  \"\n",
                "        print(f\"     {marker} {rank+1}. {reference_names[idx]} ({sims[idx].item()*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\n{'='*70}\")\n",
                "print(f\"ACCURACY: {correct}/{len(test_cards)} = {100*correct/len(test_cards):.1f}%\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**Model saved to:** `MyDrive/CardRecognition_Models/best_model.pth`\n",
                "\n",
                "**Corrupted images list:** `MyDrive/CardData/corrupted_images.json`"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}