{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition V5 - With Confusion Logging\n",
                "\n",
                "**Curriculum Learning + Live Accuracy + Confusion Tracking**\n",
                "\n",
                "- See which cards the model confuses during training\n",
                "- Uses JSON for actual card names\n",
                "- Phases: Light ‚Üí Medium ‚Üí Heavy augmentation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q timm albumentations opencv-python-headless tqdm imagehash"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Data & Card JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, zipfile, json, random\n",
                "from pathlib import Path\n",
                "from PIL import Image, ImageOps\n",
                "from tqdm.notebook import tqdm\n",
                "from collections import Counter\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "CARD_JSON = '/content/drive/MyDrive/CardData/card-flattened-with-phash.json'\n",
                "\n",
                "for d in [CHECKPOINT_DIR, DRIVE_OUTPUT]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "# Extract images\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.extracted\"):\n",
                "    print(f\"‚úì Already extracted\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(\"Extracting...\")\n",
                "    !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
                "        z.extractall(IMAGE_DIR)\n",
                "    Path(f\"{IMAGE_DIR}/.extracted\").touch()\n",
                "    print(f\"‚úì Done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate images\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.validated\"):\n",
                "    print(\"‚úì Already validated\")\n",
                "else:\n",
                "    print(\"Validating...\")\n",
                "    corrupted = []\n",
                "    for p in tqdm(list(Path(IMAGE_DIR).glob('*'))):\n",
                "        if p.suffix.lower() in ['.jpg','.jpeg','.png','.webp']:\n",
                "            try:\n",
                "                with Image.open(p) as img: img.verify()\n",
                "                with Image.open(p) as img: img.load()\n",
                "            except:\n",
                "                corrupted.append(p.name)\n",
                "                p.unlink()\n",
                "    Path(f\"{IMAGE_DIR}/.validated\").touch()\n",
                "    print(f\"‚úì Removed {len(corrupted)} corrupted\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load card JSON for names\n",
                "with open(CARD_JSON, 'r') as f:\n",
                "    all_cards = json.load(f)\n",
                "\n",
                "# Create lookup: printing_unique_id -> card info\n",
                "card_lookup = {c['printing_unique_id']: c for c in all_cards}\n",
                "\n",
                "def get_card_name(printing_id):\n",
                "    \"\"\"Get human-readable card name from printing ID.\"\"\"\n",
                "    card = card_lookup.get(printing_id, {})\n",
                "    name = card.get('name', printing_id[:20])\n",
                "    foil = card.get('foiling', '')\n",
                "    if foil and foil != 'S':  # S = Standard/non-foil\n",
                "        return f\"{name} ({foil})\"\n",
                "    return name\n",
                "\n",
                "print(f\"‚úì Loaded {len(all_cards):,} cards from JSON\")\n",
                "print(f\"   Example: {get_card_name(list(card_lookup.keys())[0])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Curriculum Augmentations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "\n",
                "def get_light_aug(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Rotate(limit=3, p=0.3),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_medium_aug(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Rotate(limit=8, p=0.4),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=3), A.MotionBlur(blur_limit=3)], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_heavy_aug(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.08), p=0.5),\n",
                "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
                "        A.OneOf([A.MotionBlur(blur_limit=5), A.GaussianBlur(blur_limit=5)], p=0.3),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5),\n",
                "        A.RandomShadow(shadow_roi=(0, 0.3, 1, 1), p=0.2),\n",
                "        A.GaussNoise(var_limit=(10, 40), p=0.3),\n",
                "        A.ImageCompression(quality_lower=70, p=0.2),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "print(\"‚úì Augmentations ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SyntheticBackground:\n",
                "    def __init__(self, output_size=(480, 640)):\n",
                "        self.output_size = output_size\n",
                "    \n",
                "    def get_random_bg(self):\n",
                "        if random.random() > 0.3:\n",
                "            color = tuple(random.randint(0, 255) for _ in range(3))\n",
                "            return np.full((*self.output_size, 3), color, dtype=np.uint8)\n",
                "        else:\n",
                "            c1, c2 = np.random.randint(0, 255, 3), np.random.randint(0, 255, 3)\n",
                "            arr = np.zeros((*self.output_size, 3), dtype=np.uint8)\n",
                "            for i in range(self.output_size[0]):\n",
                "                t = i / self.output_size[0]\n",
                "                arr[i] = (c1 * (1-t) + c2 * t).astype(np.uint8)\n",
                "            return arr\n",
                "    \n",
                "    def composite(self, card_img):\n",
                "        bg = self.get_random_bg()\n",
                "        h, w = card_img.shape[:2]\n",
                "        scale = random.uniform(0.5, 0.8)\n",
                "        new_h, new_w = int(self.output_size[0]*scale), int(self.output_size[0]*scale*w/h)\n",
                "        card = cv2.resize(card_img, (new_w, new_h))\n",
                "        y = random.randint(0, max(0, self.output_size[0]-new_h))\n",
                "        x = random.randint(0, max(0, self.output_size[1]-new_w))\n",
                "        bg[y:y+new_h, x:x+new_w] = card[:min(new_h, self.output_size[0]-y), :min(new_w, self.output_size[1]-x)]\n",
                "        return bg\n",
                "\n",
                "synth_bg = SyntheticBackground()\n",
                "print(\"‚úì SyntheticBackground ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "    def forward(self, x):\n",
                "        return F.adaptive_avg_pool2d(x.clamp(min=1e-6).pow(self.p), 1).pow(1./self.p).view(x.size(0), -1)\n",
                "\n",
                "class ColorBranch(nn.Module):\n",
                "    def __init__(self, bins=32, out_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(nn.Linear(bins*3, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, out_dim))\n",
                "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
                "        self.register_buffer('std', torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = ((x * self.std + self.mean) * 255).clamp(0, 255)\n",
                "        hists = []\n",
                "        for i in range(x.shape[0]):\n",
                "            img = x_denorm[i].permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            h = np.histogram(hsv[:,:,0], bins=self.bins, range=(0,180))[0]\n",
                "            s = np.histogram(hsv[:,:,1], bins=self.bins, range=(0,256))[0]\n",
                "            v = np.histogram(hsv[:,:,2], bins=self.bins, range=(0,256))[0]\n",
                "            hist = np.concatenate([h,s,v]).astype(np.float32)\n",
                "            hists.append(hist / (hist.sum() + 1e-8))\n",
                "        return self.fc(torch.tensor(np.stack(hists), device=x.device, dtype=torch.float32))\n",
                "\n",
                "class CardNet(nn.Module):\n",
                "    def __init__(self, emb_dim=512, color_dim=64):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=True, num_classes=0, global_pool='')\n",
                "        with torch.no_grad():\n",
                "            self.n_feat = self.backbone(torch.randn(1,3,224,224)).shape[1]\n",
                "        self.gem = GeM()\n",
                "        self.color = ColorBranch(out_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.n_feat + color_dim, emb_dim)\n",
                "        self.bn = nn.BatchNorm1d(emb_dim)\n",
                "        self.drop = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        v = self.gem(self.backbone(x))\n",
                "        c = self.color(x)\n",
                "        return F.normalize(self.drop(self.bn(self.fc(torch.cat([v, c], dim=1)))), p=2, dim=1)\n",
                "\n",
                "print(\"‚úì CardNet ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class CardDataset(Dataset):\n",
                "    def __init__(self, image_dir, rotations=[0, 90, 180, 270]):\n",
                "        self.images = sorted([f for f in Path(image_dir).iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg','.jpeg','.png','.webp']])\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        self.transform = get_light_aug()\n",
                "        self.use_bg = False\n",
                "        self.synth_bg = SyntheticBackground()\n",
                "        print(f\"Dataset: {len(self.images)} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def set_phase(self, phase):\n",
                "        if phase == 1:\n",
                "            self.transform, self.use_bg = get_light_aug(), False\n",
                "            print(\"üìö Phase 1: Light augmentation\")\n",
                "        elif phase == 2:\n",
                "            self.transform, self.use_bg = get_medium_aug(), False\n",
                "            print(\"üìö Phase 2: Medium augmentation\")\n",
                "        else:\n",
                "            self.transform, self.use_bg = get_heavy_aug(), True\n",
                "            print(\"üìö Phase 3: Heavy + backgrounds\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rot = self.samples[idx]\n",
                "        try:\n",
                "            img = np.array(Image.open(self.images[img_idx]).convert('RGB'))\n",
                "            if rot == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rot == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rot == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            if self.use_bg and random.random() > 0.5:\n",
                "                img = self.synth_bg.composite(img)\n",
                "            return self.transform(image=img)['image'], img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples)-1))\n",
                "    \n",
                "    def get_num_classes(self): return len(self.images)\n",
                "    def get_printing_id(self, idx): return self.images[idx].stem\n",
                "\n",
                "print(\"‚úì CardDataset ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Confusion Tracker"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConfusionTracker:\n",
                "    \"\"\"Track which cards get confused during training.\"\"\"\n",
                "    \n",
                "    def __init__(self, get_name_fn):\n",
                "        self.confusions = Counter()  # (actual, predicted) -> count\n",
                "        self.get_name = get_name_fn\n",
                "    \n",
                "    def log_confusion(self, actual_id, predicted_id):\n",
                "        \"\"\"Log a confusion between two cards.\"\"\"\n",
                "        if actual_id != predicted_id:\n",
                "            actual_name = self.get_name(actual_id)\n",
                "            pred_name = self.get_name(predicted_id)\n",
                "            self.confusions[(actual_name, pred_name)] += 1\n",
                "    \n",
                "    def get_top_confusions(self, n=10):\n",
                "        \"\"\"Get most common confusion pairs.\"\"\"\n",
                "        return self.confusions.most_common(n)\n",
                "    \n",
                "    def print_report(self, n=10):\n",
                "        \"\"\"Print confusion report.\"\"\"\n",
                "        top = self.get_top_confusions(n)\n",
                "        if not top:\n",
                "            print(\"   No confusions logged yet!\")\n",
                "            return\n",
                "        \n",
                "        print(f\"\\n   üîÑ TOP {min(n, len(top))} CONFUSIONS:\")\n",
                "        for (actual, pred), count in top:\n",
                "            print(f\"      '{actual}' ‚Üí '{pred}' ({count}x)\")\n",
                "    \n",
                "    def reset(self):\n",
                "        self.confusions.clear()\n",
                "\n",
                "print(\"‚úì ConfusionTracker ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AccuracyMonitor:\n",
                "    \"\"\"Monitor accuracy and track confusions.\"\"\"\n",
                "    \n",
                "    def __init__(self, model, dataset, device, confusion_tracker, n_test=100):\n",
                "        self.model = model\n",
                "        self.dataset = dataset\n",
                "        self.device = device\n",
                "        self.tracker = confusion_tracker\n",
                "        self.n_test = n_test\n",
                "        self.transform = get_val_transforms()\n",
                "        self.ref_embeddings = None\n",
                "        self.ref_ids = None\n",
                "    \n",
                "    def build_references(self):\n",
                "        self.model.eval()\n",
                "        embeddings, ids = [], []\n",
                "        with torch.no_grad():\n",
                "            for img_path in self.dataset.images:\n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    emb = self.model(self.transform(image=img)['image'].unsqueeze(0).to(self.device))\n",
                "                    embeddings.append(emb.cpu())\n",
                "                    ids.append(img_path.stem)\n",
                "                except: pass\n",
                "        self.ref_embeddings = torch.cat(embeddings, dim=0)\n",
                "        self.ref_ids = ids\n",
                "    \n",
                "    def compute_accuracy(self):\n",
                "        if self.ref_embeddings is None:\n",
                "            self.build_references()\n",
                "        \n",
                "        self.model.eval()\n",
                "        self.tracker.reset()  # Reset confusions for this check\n",
                "        \n",
                "        test_indices = random.sample(range(len(self.dataset.images)), \n",
                "                                      min(self.n_test, len(self.dataset.images)))\n",
                "        top1, top5, top10 = 0, 0, 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for idx in test_indices:\n",
                "                img_path = self.dataset.images[idx]\n",
                "                actual_id = img_path.stem\n",
                "                \n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    query = self.model(self.transform(image=img)['image'].unsqueeze(0).to(self.device)).cpu()\n",
                "                    sims = F.cosine_similarity(query, self.ref_embeddings)\n",
                "                    top_idx = sims.argsort(descending=True)[:10]\n",
                "                    top_ids = [self.ref_ids[i] for i in top_idx]\n",
                "                    \n",
                "                    # Track accuracy\n",
                "                    if actual_id == top_ids[0]: top1 += 1\n",
                "                    if actual_id in top_ids[:5]: top5 += 1\n",
                "                    if actual_id in top_ids[:10]: top10 += 1\n",
                "                    \n",
                "                    # Log confusion if wrong\n",
                "                    if actual_id != top_ids[0]:\n",
                "                        self.tracker.log_confusion(actual_id, top_ids[0])\n",
                "                except: pass\n",
                "        \n",
                "        n = len(test_indices)\n",
                "        return {'top1': 100*top1/n, 'top5': 100*top5/n, 'top10': 100*top10/n}\n",
                "\n",
                "print(\"‚úì AccuracyMonitor ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, emb_dim, scale=64.0, margin=0.4):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, emb_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, emb, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cos = F.linear(emb, W)\n",
                "        one_hot = torch.zeros_like(cos).scatter_(1, labels.view(-1,1), 1.0)\n",
                "        return F.cross_entropy((cos - one_hot * self.margin) * self.scale, labels)\n",
                "\n",
                "PHASES = {1: (1, 10), 2: (11, 20), 3: (21, 100)}\n",
                "\n",
                "CONFIG = {\n",
                "    'epochs': 60,\n",
                "    'batch_size': 64,\n",
                "    'lr': 1e-3,\n",
                "    'weight_decay': 1e-4,\n",
                "    'emb_dim': 512,\n",
                "    'patience': 15,\n",
                "    'unfreeze_epoch': 8,\n",
                "    'check_interval': 5,\n",
                "}\n",
                "print(\"‚úì Config:\", CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataset and loaders\n",
                "train_ds = CardDataset(IMAGE_DIR)\n",
                "val_ds = CardDataset(IMAGE_DIR, rotations=[0])\n",
                "val_ds.transform = get_val_transforms()\n",
                "val_ds.use_bg = False\n",
                "\n",
                "indices = np.random.permutation(len(train_ds.images))\n",
                "split = int(0.85 * len(train_ds.images))\n",
                "train_idx, val_idx = set(indices[:split]), set(indices[split:])\n",
                "\n",
                "train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "\n",
                "train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                          batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, \n",
                "                          pin_memory=True, drop_last=True)\n",
                "val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                        batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "num_classes = train_ds.get_num_classes()\n",
                "print(f\"‚úì Train: {len(train_samples):,} | Val: {len(val_samples):,} | Classes: {num_classes:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = CardNet(emb_dim=CONFIG['emb_dim']).to(device)\n",
                "\n",
                "for p in model.backbone.parameters(): p.requires_grad = False\n",
                "print(\"‚úì Backbone frozen\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['emb_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "# Confusion tracker with card name lookup\n",
                "confusion_tracker = ConfusionTracker(get_card_name)\n",
                "accuracy_monitor = AccuracyMonitor(model, train_ds, device, confusion_tracker, n_test=100)\n",
                "\n",
                "print(\"‚úì Ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Training with Confusion Logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': [], 'top1': [], 'top5': []}\n",
                "current_phase = 0\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"TRAINING WITH CONFUSION LOGGING\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    # Update curriculum phase\n",
                "    for phase, (start, end) in PHASES.items():\n",
                "        if start <= epoch <= end and phase != current_phase:\n",
                "            current_phase = phase\n",
                "            train_ds.set_phase(phase)\n",
                "            accuracy_monitor.ref_embeddings = None  # Rebuild refs\n",
                "            break\n",
                "    \n",
                "    # Unfreeze backbone\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters(): p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['lr']/10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [P{current_phase}]\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    # Accuracy check with confusion logging\n",
                "    if epoch % CONFIG['check_interval'] == 0:\n",
                "        acc = accuracy_monitor.compute_accuracy()\n",
                "        history['top1'].append(acc['top1'])\n",
                "        history['top5'].append(acc['top5'])\n",
                "        print(f\"\\nEpoch {epoch}: Loss={train_loss:.2f}/{val_loss:.2f} | \"\n",
                "              f\"Top-1: {acc['top1']:.1f}% | Top-5: {acc['top5']:.1f}% | Top-10: {acc['top10']:.1f}%\")\n",
                "        confusion_tracker.print_report(n=5)\n",
                "    else:\n",
                "        print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    # Save best\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "                    'val_loss': val_loss, 'num_classes': num_classes}, RESUME_PATH)\n",
                "        print(\"  üíæ Saved\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(\"\\n‚ö†Ô∏è Early stop!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Done! Best: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "axes[0].plot(history['train'], label='Train')\n",
                "axes[0].plot(history['val'], label='Val')\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
                "axes[0].set_title('Loss')\n",
                "\n",
                "if history['top1']:\n",
                "    x = list(range(CONFIG['check_interval'], len(history['top1'])*CONFIG['check_interval']+1, CONFIG['check_interval']))\n",
                "    axes[1].plot(x, history['top1'], 'o-', label='Top-1')\n",
                "    axes[1].plot(x, history['top5'], 's-', label='Top-5')\n",
                "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy %'); axes[1].legend()\n",
                "    axes[1].set_title('Identification Accuracy')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "for f in ['best_model.pth', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "        print(f\"‚úì Saved {f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Final Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building final embeddings...\")\n",
                "ckpt = torch.load(RESUME_PATH)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "ref_embeddings, ref_names = [], []\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            emb = model(get_val_transforms()(image=img)['image'].unsqueeze(0).to(device))\n",
                "            ref_embeddings.append(emb.cpu())\n",
                "            ref_names.append(img_path.stem)\n",
                "        except: pass\n",
                "\n",
                "ref_embeddings = torch.cat(ref_embeddings, dim=0)\n",
                "print(f\"‚úì {len(ref_embeddings):,} embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final accuracy with confusion report\n",
                "final_tracker = ConfusionTracker(get_card_name)\n",
                "final_monitor = AccuracyMonitor(model, train_ds, device, final_tracker, n_test=200)\n",
                "final_acc = final_monitor.compute_accuracy()\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"FINAL ACCURACY (200 test cards)\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Top-1:  {final_acc['top1']:.1f}%\")\n",
                "print(f\"Top-5:  {final_acc['top5']:.1f}%\")\n",
                "print(f\"Top-10: {final_acc['top10']:.1f}%\")\n",
                "final_tracker.print_report(n=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with upload\n",
                "import imagehash\n",
                "from google.colab import files\n",
                "\n",
                "name_to_printings = {}\n",
                "for c in all_cards:\n",
                "    name = c.get('name', '')\n",
                "    if name not in name_to_printings:\n",
                "        name_to_printings[name] = []\n",
                "    name_to_printings[name].append({'id': c['printing_unique_id'], 'set': c.get('set_id',''), \n",
                "                                    'foil': c.get('foiling',''), 'phash': c.get('image_phash','')[:64]})\n",
                "\n",
                "def identify(img_path):\n",
                "    pil = ImageOps.autocontrast(Image.open(img_path).convert('RGB'), cutoff=1)\n",
                "    arr = np.array(pil)\n",
                "    with torch.no_grad():\n",
                "        q = model(get_val_transforms()(image=arr)['image'].unsqueeze(0).to(device)).cpu()\n",
                "    sims = F.cosine_similarity(q, ref_embeddings)\n",
                "    top_id = ref_names[sims.argmax().item()]\n",
                "    name = get_card_name(top_id)\n",
                "    conf = sims.max().item()\n",
                "    \n",
                "    # pHash for exact printing\n",
                "    printings = name_to_printings.get(card_lookup.get(top_id, {}).get('name', ''), [])\n",
                "    if printings:\n",
                "        qh = imagehash.phash(pil, hash_size=16)\n",
                "        best = min(printings, key=lambda p: (imagehash.hex_to_hash(p['phash'])-qh) if p['phash'] else 999)\n",
                "        return {'name': name, 'conf': conf, 'set': best['set'], 'foil': best['foil']}\n",
                "    return {'name': name, 'conf': conf}\n",
                "\n",
                "print('Upload images to test:')\n",
                "for f in files.upload().keys():\n",
                "    r = identify(f)\n",
                "    print(f\"\\nüé¥ {r['name']} ({r['conf']*100:.0f}%)\")\n",
                "    print(f\"   Set: {r.get('set','')} | Foil: {r.get('foil','')}\")\n",
                "    plt.imshow(Image.open(f)); plt.axis('off'); plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**New: Confusion Logging** - See which cards the model struggles with!\n",
                "\n",
                "Example output:\n",
                "```\n",
                "Epoch 10: Top-1: 75% | Top-5: 95%\n",
                "   üîÑ TOP 5 CONFUSIONS:\n",
                "      'Razor Reflex (Red)' ‚Üí 'Razor Reflex (Blue)' (3x)\n",
                "      'Sink Below' ‚Üí 'Sink Below (Yellow)' (2x)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}