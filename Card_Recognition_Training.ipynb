{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training\n",
                "\n",
                "**Train on Colab ‚Üí Deploy on Jetson Nano**\n",
                "\n",
                "## Features:\n",
                "- MobileNetV3-Small backbone (60 FPS on Jetson)\n",
                "- Color histogram branch (distinguishes similar cards)\n",
                "- CosFace loss (stable for fine-grained recognition)\n",
                "- On-the-fly augmentation (rotation + sim-to-real)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup - Clone from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== YOUR GITHUB REPO =====\n",
                "GITHUB_REPO = \"Krishan552Patel/Card-recognition-fab\"\n",
                "BRANCH = \"master\"\n",
                "\n",
                "# Clone repository\n",
                "import os\n",
                "\n",
                "WORK_DIR = \"/content/card_recognition\"\n",
                "if os.path.exists(WORK_DIR):\n",
                "    !rm -rf {WORK_DIR}\n",
                "\n",
                "!git clone https://github.com/{GITHUB_REPO}.git {WORK_DIR}\n",
                "os.chdir(WORK_DIR)\n",
                "print(f\"\\n‚úì Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q timm albumentations opencv-python-headless tqdm tensorboard imagehash"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Card Data from Google Drive\n",
                "\n",
                "**Your card data should be:**\n",
                "```\n",
                "MyDrive/CardData/card_images.zip\n",
                "```\n",
                "\n",
                "This contains your original card scans (one image per card).\n",
                "Rotations and augmentations are created on-the-fly during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== CONFIGURE YOUR DATA PATH =====\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "\n",
                "# Local destination\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    print(f\"Found: {ZIP_PATH}\")\n",
                "    print(\"Extracting... (this may take a few minutes)\")\n",
                "    \n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "        zip_ref.extractall(IMAGE_DIR)\n",
                "    \n",
                "    # Count images\n",
                "    images = [f for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
                "    print(f\"\\n‚úì Extracted {len(images):,} card images\")\n",
                "else:\n",
                "    print(f\"‚ùå ZIP not found: {ZIP_PATH}\")\n",
                "    print(\"Please upload card_images.zip to Google Drive/CardData/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    \"\"\"Generalized Mean Pooling.\"\"\"\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.clamp(min=self.eps).pow(self.p)\n",
                "        x = F.adaptive_avg_pool2d(x, 1).pow(1.0 / self.p)\n",
                "        return x.view(x.size(0), -1)\n",
                "\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    \"\"\"Explicit color feature extraction.\"\"\"\n",
                "    \n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(bins * 3, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, output_dim)\n",
                "        )\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = (x * self.std + self.mean) * 255\n",
                "        x_denorm = x_denorm.clamp(0, 255)\n",
                "        \n",
                "        batch_size = x.shape[0]\n",
                "        histograms = []\n",
                "        \n",
                "        for i in range(batch_size):\n",
                "            img = x_denorm[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            \n",
                "            h_hist = np.histogram(hsv[:,:,0], bins=self.bins, range=(0, 180))[0]\n",
                "            s_hist = np.histogram(hsv[:,:,1], bins=self.bins, range=(0, 256))[0]\n",
                "            v_hist = np.histogram(hsv[:,:,2], bins=self.bins, range=(0, 256))[0]\n",
                "            \n",
                "            hist = np.concatenate([h_hist, s_hist, v_hist]).astype(np.float32)\n",
                "            hist = hist / (hist.sum() + 1e-8)\n",
                "            histograms.append(hist)\n",
                "        \n",
                "        return self.fc(torch.tensor(np.stack(histograms), device=x.device, dtype=torch.float32))\n",
                "\n",
                "\n",
                "class CardEmbeddingNetV2(nn.Module):\n",
                "    \"\"\"Color-aware card embedding network.\"\"\"\n",
                "    \n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained,\n",
                "                                          num_classes=0, global_pool='')\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        \n",
                "        self.gem = GeM(p=3.0)\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        combined = torch.cat([visual, color], dim=1)\n",
                "        embedding = self.dropout(self.bn(self.fc(combined)))\n",
                "        return F.normalize(embedding, p=2, dim=1)\n",
                "\n",
                "\n",
                "# Test\n",
                "model = CardEmbeddingNetV2()\n",
                "out = model(torch.randn(2, 3, 224, 224))\n",
                "print(f\"‚úì Model output: {out.shape}, Params: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ CosFace Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    \"\"\"CosFace: More stable than ArcFace for card recognition.\"\"\"\n",
                "    \n",
                "    def __init__(self, num_classes, embedding_dim, scale=30.0, margin=0.35):\n",
                "        super().__init__()\n",
                "        self.scale = scale\n",
                "        self.margin = margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine)\n",
                "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        output = (cosine - one_hot * self.margin) * self.scale\n",
                "        return F.cross_entropy(output, labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset with On-the-Fly Rotation + Augmentation\n",
                "\n",
                "Creates 4 rotations (0¬∞, 90¬∞, 180¬∞, 270¬∞) and applies sim-to-real augmentation **during training** - no need to pre-generate!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "import random\n",
                "\n",
                "\n",
                "def get_train_transforms(size=224):\n",
                "    \"\"\"Sim-to-Real + random rotation.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Affine(scale=(0.97, 1.03), rotate=(-2, 2), p=0.3),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=(3, 5)), A.MotionBlur(blur_limit=(3, 5))], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),\n",
                "        A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=10, val_shift_limit=10, p=0.2),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "\n",
                "class CardDatasetWithRotation(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset that creates 4x rotations on-the-fly.\n",
                "    \n",
                "    Input: Flat folder of card images (one per card)\n",
                "    Output: Each card appears 4 times (0¬∞, 90¬∞, 180¬∞, 270¬∞)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, image_dir, transform=None, rotations=[0, 90, 180, 270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.rotations = rotations\n",
                "        \n",
                "        self.images = sorted([\n",
                "            f for f in self.image_dir.iterdir()\n",
                "            if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']\n",
                "        ])\n",
                "        \n",
                "        self.num_cards = len(self.images)\n",
                "        \n",
                "        self.samples = []\n",
                "        for img_idx, img_path in enumerate(self.images):\n",
                "            for rot in self.rotations:\n",
                "                self.samples.append((img_idx, rot))\n",
                "        \n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(self.rotations)} rotations = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        img_path = self.images[img_idx]\n",
                "        \n",
                "        img = np.array(Image.open(img_path).convert('RGB'))\n",
                "        \n",
                "        if rotation == 90:\n",
                "            img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "        elif rotation == 180:\n",
                "            img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "        elif rotation == 270:\n",
                "            img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "        \n",
                "        if self.transform:\n",
                "            img = self.transform(image=img)['image']\n",
                "        \n",
                "        return img, img_idx\n",
                "    \n",
                "    def get_num_classes(self):\n",
                "        return self.num_cards\n",
                "\n",
                "\n",
                "def create_dataloaders(image_dir, batch_size=64, num_workers=2, val_split=0.15):\n",
                "    train_ds = CardDatasetWithRotation(image_dir, get_train_transforms())\n",
                "    val_ds = CardDatasetWithRotation(image_dir, get_val_transforms(), rotations=[0])\n",
                "    \n",
                "    n_cards = train_ds.num_cards\n",
                "    indices = np.random.permutation(n_cards)\n",
                "    split = int((1 - val_split) * n_cards)\n",
                "    \n",
                "    train_card_indices = set(indices[:split])\n",
                "    val_card_indices = set(indices[split:])\n",
                "    \n",
                "    train_sample_indices = [i for i, (card_idx, _) in enumerate(train_ds.samples) if card_idx in train_card_indices]\n",
                "    val_sample_indices = [i for i, (card_idx, _) in enumerate(val_ds.samples) if card_idx in val_card_indices]\n",
                "    \n",
                "    train_loader = DataLoader(\n",
                "        torch.utils.data.Subset(train_ds, train_sample_indices),\n",
                "        batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
                "        pin_memory=True, drop_last=True\n",
                "    )\n",
                "    val_loader = DataLoader(\n",
                "        torch.utils.data.Subset(val_ds, val_sample_indices),\n",
                "        batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
                "    )\n",
                "    \n",
                "    print(f\"Train: {len(train_sample_indices)} samples ({len(train_card_indices)} cards)\")\n",
                "    print(f\"Val: {len(val_sample_indices)} samples ({len(val_card_indices)} cards)\")\n",
                "    \n",
                "    return train_loader, val_loader, train_ds.get_num_classes()\n",
                "\n",
                "\n",
                "# Test\n",
                "if os.path.exists(IMAGE_DIR):\n",
                "    train_loader, val_loader, num_classes = create_dataloaders(IMAGE_DIR, batch_size=4)\n",
                "    print(f\"‚úì Classes: {num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'epochs': 100,\n",
                "    'batch_size': 64,\n",
                "    'learning_rate': 1e-3,\n",
                "    'weight_decay': 1e-4,\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 15,\n",
                "    'unfreeze_epoch': 6,\n",
                "}\n",
                "\n",
                "print(\"Configuration:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Train!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.cuda.amp import autocast, GradScaler\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "CHECKPOINT_DIR = '/content/card_recognition/checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "train_loader, val_loader, num_classes = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "\n",
                "model = CardEmbeddingNetV2(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "for p in model.backbone.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = GradScaler()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(f\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters():\n",
                "            p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate'] / 10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        with autocast():\n",
                "            loss = criterion(model(images), labels)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    \n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "            'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG\n",
                "        }, f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "        print(f\"  üíæ Saved best model\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(f\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best val loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Export & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history['train'], label='Train')\n",
                "plt.plot(history['val'], label='Val')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.title('Training Progress')\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
                "onnx_path = f\"{CHECKPOINT_DIR}/card_recognition.onnx\"\n",
                "\n",
                "torch.onnx.export(model, dummy, onnx_path,\n",
                "                  input_names=['image'], output_names=['embedding'],\n",
                "                  dynamic_axes={'image': {0: 'batch'}, 'embedding': {0: 'batch'}},\n",
                "                  opset_version=11)\n",
                "\n",
                "print(f\"‚úì ONNX: {onnx_path} ({os.path.getsize(onnx_path)/1024/1024:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Google Drive\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
                "\n",
                "import shutil\n",
                "for f in ['best_model.pth', 'card_recognition.onnx', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "\n",
                "print(f\"\\n‚úì Saved to: {DRIVE_OUTPUT}\")\n",
                "!ls -lh {DRIVE_OUTPUT}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Done!\n",
                "\n",
                "**Files saved to Google Drive:**\n",
                "- `best_model.pth` - PyTorch checkpoint\n",
                "- `card_recognition.onnx` - For Jetson Nano\n",
                "- `training.png` - Training curves\n",
                "\n",
                "**Next: Deploy to Jetson Nano**\n",
                "```bash\n",
                "trtexec --onnx=card_recognition.onnx --saveEngine=card.engine --fp16\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}