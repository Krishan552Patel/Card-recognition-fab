{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition V4 - Curriculum Learning\n",
                "\n",
                "**Progressive Training with Live Accuracy Monitoring**\n",
                "\n",
                "- Phase 1: Clean cards, light augmentation\n",
                "- Phase 2: Medium augmentation\n",
                "- Phase 3: Full augmentation + backgrounds\n",
                "- Live accuracy monitoring every 5 epochs\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q timm albumentations opencv-python-headless tqdm imagehash"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Extract Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, zipfile, json, random\n",
                "from pathlib import Path\n",
                "from PIL import Image, ImageOps\n",
                "from tqdm.notebook import tqdm\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "CARD_JSON = '/content/drive/MyDrive/CardData/card-flattened-with-phash.json'\n",
                "\n",
                "for d in [CHECKPOINT_DIR, DRIVE_OUTPUT]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.extracted\"):\n",
                "    print(f\"‚úì Already extracted\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(\"Extracting...\")\n",
                "    !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
                "        z.extractall(IMAGE_DIR)\n",
                "    Path(f\"{IMAGE_DIR}/.extracted\").touch()\n",
                "    print(f\"‚úì Done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(f\"{IMAGE_DIR}/.validated\"):\n",
                "    print(\"‚úì Already validated\")\n",
                "else:\n",
                "    print(\"Validating...\")\n",
                "    corrupted = []\n",
                "    for p in tqdm(list(Path(IMAGE_DIR).glob('*'))):\n",
                "        if p.suffix.lower() in ['.jpg','.jpeg','.png','.webp']:\n",
                "            try:\n",
                "                with Image.open(p) as img: img.verify()\n",
                "                with Image.open(p) as img: img.load()\n",
                "            except:\n",
                "                corrupted.append(p.name)\n",
                "                p.unlink()\n",
                "    Path(f\"{IMAGE_DIR}/.validated\").touch()\n",
                "    print(f\"‚úì Removed {len(corrupted)} corrupted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Curriculum: Light ‚Üí Medium ‚Üí Full Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "\n",
                "def get_light_augmentations(size=224):\n",
                "    \"\"\"Phase 1: Minimal augmentation - learn basic card features.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Rotate(limit=3, p=0.3),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_medium_augmentations(size=224):\n",
                "    \"\"\"Phase 2: Moderate augmentation - add camera-like effects.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
                "        A.Rotate(limit=8, p=0.4),\n",
                "        A.OneOf([A.GaussianBlur(blur_limit=3), A.MotionBlur(blur_limit=3)], p=0.2),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),\n",
                "        A.GaussNoise(var_limit=(5, 20), p=0.2),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_heavy_augmentations(size=224):\n",
                "    \"\"\"Phase 3: Heavy augmentation with backgrounds.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Perspective(scale=(0.02, 0.08), p=0.5),\n",
                "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
                "        A.OneOf([A.MotionBlur(blur_limit=5), A.GaussianBlur(blur_limit=5)], p=0.3),\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5),\n",
                "        A.RandomShadow(shadow_roi=(0, 0.3, 1, 1), p=0.2),\n",
                "        A.GaussNoise(var_limit=(10, 40), p=0.3),\n",
                "        A.ImageCompression(quality_lower=70, p=0.2),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "print(\"‚úì Curriculum augmentations ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SyntheticBackground:\n",
                "    def __init__(self, output_size=(480, 640)):\n",
                "        self.output_size = output_size\n",
                "    \n",
                "    def solid_color(self):\n",
                "        color = tuple(random.randint(0, 255) for _ in range(3))\n",
                "        return np.full((*self.output_size, 3), color, dtype=np.uint8)\n",
                "    \n",
                "    def gradient(self):\n",
                "        c1 = np.array([random.randint(0, 255) for _ in range(3)])\n",
                "        c2 = np.array([random.randint(0, 255) for _ in range(3)])\n",
                "        arr = np.zeros((*self.output_size, 3), dtype=np.uint8)\n",
                "        for i in range(self.output_size[0]):\n",
                "            t = i / self.output_size[0]\n",
                "            arr[i] = (c1 * (1 - t) + c2 * t).astype(np.uint8)\n",
                "        return arr\n",
                "    \n",
                "    def get_random_bg(self):\n",
                "        return self.solid_color() if random.random() > 0.3 else self.gradient()\n",
                "    \n",
                "    def composite(self, card_img):\n",
                "        bg = self.get_random_bg()\n",
                "        h, w = card_img.shape[:2]\n",
                "        scale = random.uniform(0.5, 0.8)\n",
                "        new_h = int(self.output_size[0] * scale)\n",
                "        new_w = int(new_h * w / h)\n",
                "        card_resized = cv2.resize(card_img, (new_w, new_h))\n",
                "        \n",
                "        max_y = max(0, self.output_size[0] - new_h)\n",
                "        max_x = max(0, self.output_size[1] - new_w)\n",
                "        y = random.randint(0, max_y) if max_y > 0 else 0\n",
                "        x = random.randint(0, max_x) if max_x > 0 else 0\n",
                "        \n",
                "        y_end = min(y + new_h, self.output_size[0])\n",
                "        x_end = min(x + new_w, self.output_size[1])\n",
                "        bg[y:y_end, x:x_end] = card_resized[:y_end-y, :x_end-x]\n",
                "        return bg\n",
                "\n",
                "synth_bg = SyntheticBackground()\n",
                "print(\"‚úì SyntheticBackground ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return F.adaptive_avg_pool2d(x.clamp(min=self.eps).pow(self.p), 1).pow(1./self.p).view(x.size(0), -1)\n",
                "\n",
                "class ColorBranch(nn.Module):\n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(nn.Linear(bins*3, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, output_dim))\n",
                "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
                "        self.register_buffer('std', torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = ((x * self.std + self.mean) * 255).clamp(0, 255)\n",
                "        hists = []\n",
                "        for i in range(x.shape[0]):\n",
                "            img = x_denorm[i].permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            h = np.histogram(hsv[:,:,0], bins=self.bins, range=(0,180))[0]\n",
                "            s = np.histogram(hsv[:,:,1], bins=self.bins, range=(0,256))[0]\n",
                "            v = np.histogram(hsv[:,:,2], bins=self.bins, range=(0,256))[0]\n",
                "            hist = np.concatenate([h,s,v]).astype(np.float32)\n",
                "            hists.append(hist / (hist.sum() + 1e-8))\n",
                "        return self.fc(torch.tensor(np.stack(hists), device=x.device, dtype=torch.float32))\n",
                "\n",
                "class CardEmbeddingNet(nn.Module):\n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained, num_classes=0, global_pool='')\n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1,3,224,224)).shape[1]\n",
                "        self.gem = GeM()\n",
                "        self.color_branch = ColorBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        return F.normalize(self.dropout(self.bn(self.fc(torch.cat([visual, color], dim=1)))), p=2, dim=1)\n",
                "\n",
                "print(\"‚úì Model ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset with Curriculum"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class CurriculumDataset(Dataset):\n",
                "    def __init__(self, image_dir, rotations=[0, 90, 180, 270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.rotations = rotations\n",
                "        self.images = sorted([f for f in self.image_dir.iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg','.jpeg','.png','.webp']])\n",
                "        self.num_cards = len(self.images)\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        \n",
                "        # Curriculum settings (updated by trainer)\n",
                "        self.transform = get_light_augmentations()\n",
                "        self.use_backgrounds = False\n",
                "        self.synth_bg = SyntheticBackground()\n",
                "        \n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def set_phase(self, phase):\n",
                "        \"\"\"Update augmentation based on training phase.\"\"\"\n",
                "        if phase == 1:\n",
                "            self.transform = get_light_augmentations()\n",
                "            self.use_backgrounds = False\n",
                "            print(\"üìö Phase 1: Light augmentation, no backgrounds\")\n",
                "        elif phase == 2:\n",
                "            self.transform = get_medium_augmentations()\n",
                "            self.use_backgrounds = False\n",
                "            print(\"üìö Phase 2: Medium augmentation, no backgrounds\")\n",
                "        else:\n",
                "            self.transform = get_heavy_augmentations()\n",
                "            self.use_backgrounds = True\n",
                "            print(\"üìö Phase 3: Heavy augmentation WITH backgrounds\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        try:\n",
                "            img = np.array(Image.open(self.images[img_idx]).convert('RGB'))\n",
                "            \n",
                "            if rotation == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rotation == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rotation == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            \n",
                "            if self.use_backgrounds and random.random() > 0.5:\n",
                "                img = self.synth_bg.composite(img)\n",
                "            \n",
                "            img = self.transform(image=img)['image']\n",
                "            return img, img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples)-1))\n",
                "    \n",
                "    def get_num_classes(self): return self.num_cards\n",
                "\n",
                "print(\"‚úì CurriculumDataset ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Live Accuracy Monitor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AccuracyMonitor:\n",
                "    \"\"\"Monitor actual identification accuracy during training.\"\"\"\n",
                "    \n",
                "    def __init__(self, model, images, device, n_test=50):\n",
                "        self.model = model\n",
                "        self.images = images\n",
                "        self.device = device\n",
                "        self.n_test = n_test\n",
                "        self.transform = get_val_transforms()\n",
                "        self.reference_embeddings = None\n",
                "        self.reference_names = None\n",
                "    \n",
                "    def build_references(self):\n",
                "        \"\"\"Build reference embeddings from all cards.\"\"\"\n",
                "        self.model.eval()\n",
                "        embeddings = []\n",
                "        names = []\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for img_path in self.images:\n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    tensor = self.transform(image=img)['image'].unsqueeze(0).to(self.device)\n",
                "                    emb = self.model(tensor)\n",
                "                    embeddings.append(emb.cpu())\n",
                "                    names.append(img_path.stem)\n",
                "                except:\n",
                "                    pass\n",
                "        \n",
                "        self.reference_embeddings = torch.cat(embeddings, dim=0)\n",
                "        self.reference_names = names\n",
                "    \n",
                "    def compute_accuracy(self):\n",
                "        \"\"\"Test identification accuracy on random subset.\"\"\"\n",
                "        if self.reference_embeddings is None:\n",
                "            self.build_references()\n",
                "        \n",
                "        self.model.eval()\n",
                "        test_indices = random.sample(range(len(self.images)), min(self.n_test, len(self.images)))\n",
                "        \n",
                "        top1, top5, top10 = 0, 0, 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for idx in test_indices:\n",
                "                img_path = self.images[idx]\n",
                "                actual_name = img_path.stem\n",
                "                \n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    tensor = self.transform(image=img)['image'].unsqueeze(0).to(self.device)\n",
                "                    query_emb = self.model(tensor).cpu()\n",
                "                    \n",
                "                    sims = F.cosine_similarity(query_emb, self.reference_embeddings)\n",
                "                    top_indices = sims.argsort(descending=True)[:10]\n",
                "                    top_names = [self.reference_names[i] for i in top_indices]\n",
                "                    \n",
                "                    if actual_name == top_names[0]: top1 += 1\n",
                "                    if actual_name in top_names[:5]: top5 += 1\n",
                "                    if actual_name in top_names[:10]: top10 += 1\n",
                "                except:\n",
                "                    pass\n",
                "        \n",
                "        n = len(test_indices)\n",
                "        return {\n",
                "            'top1': 100 * top1 / n,\n",
                "            'top5': 100 * top5 / n,\n",
                "            'top10': 100 * top10 / n\n",
                "        }\n",
                "\n",
                "print(\"‚úì AccuracyMonitor ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, embedding_dim, scale=64.0, margin=0.4):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1,1), 1.0)\n",
                "        return F.cross_entropy((cosine - one_hot * self.margin) * self.scale, labels)\n",
                "\n",
                "# Curriculum phases\n",
                "PHASE_EPOCHS = {\n",
                "    1: (1, 10),    # Epochs 1-10: Light\n",
                "    2: (11, 20),   # Epochs 11-20: Medium  \n",
                "    3: (21, 100),  # Epochs 21+: Heavy + backgrounds\n",
                "}\n",
                "\n",
                "CONFIG = {\n",
                "    'epochs': 60,\n",
                "    'batch_size': 64,\n",
                "    'learning_rate': 1e-3,  # Higher for phase 1\n",
                "    'weight_decay': 1e-4,\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 15,\n",
                "    'unfreeze_epoch': 8,\n",
                "    'accuracy_check_interval': 5,  # Check accuracy every N epochs\n",
                "}\n",
                "print(\"‚úì Config:\", CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataset and loaders\n",
                "train_ds = CurriculumDataset(IMAGE_DIR)\n",
                "val_ds = CurriculumDataset(IMAGE_DIR, rotations=[0])\n",
                "val_ds.transform = get_val_transforms()\n",
                "val_ds.use_backgrounds = False\n",
                "\n",
                "indices = np.random.permutation(train_ds.num_cards)\n",
                "split = int(0.85 * train_ds.num_cards)\n",
                "train_idx, val_idx = set(indices[:split]), set(indices[split:])\n",
                "\n",
                "train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "\n",
                "train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                          batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, \n",
                "                          pin_memory=True, drop_last=True)\n",
                "val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                        batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "num_classes = train_ds.get_num_classes()\n",
                "print(f\"‚úì Train: {len(train_samples):,} | Val: {len(val_samples):,} | Classes: {num_classes:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = CardEmbeddingNet(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "for p in model.backbone.parameters(): p.requires_grad = False\n",
                "print(\"‚úì Backbone frozen\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "# Accuracy monitor\n",
                "accuracy_monitor = AccuracyMonitor(model, train_ds.images, device, n_test=100)\n",
                "\n",
                "print(\"‚úì Ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Training Loop with Live Monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': [], 'top1': [], 'top5': []}\n",
                "current_phase = 0\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"TRAINING WITH CURRICULUM LEARNING\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    # Update curriculum phase\n",
                "    for phase, (start, end) in PHASE_EPOCHS.items():\n",
                "        if start <= epoch <= end and phase != current_phase:\n",
                "            current_phase = phase\n",
                "            train_ds.set_phase(phase)\n",
                "            # Rebuild references when phase changes\n",
                "            accuracy_monitor.reference_embeddings = None\n",
                "            break\n",
                "    \n",
                "    # Unfreeze backbone\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters(): p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate']/10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "    \n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [P{current_phase}]\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    # Live accuracy check\n",
                "    if epoch % CONFIG['accuracy_check_interval'] == 0:\n",
                "        acc = accuracy_monitor.compute_accuracy()\n",
                "        history['top1'].append(acc['top1'])\n",
                "        history['top5'].append(acc['top5'])\n",
                "        print(f\"Epoch {epoch}: Loss={train_loss:.2f}/{val_loss:.2f} | \"\n",
                "              f\"Top-1: {acc['top1']:.1f}% | Top-5: {acc['top5']:.1f}% | Top-10: {acc['top10']:.1f}%\")\n",
                "    else:\n",
                "        print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    # Save best\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "                    'val_loss': val_loss, 'num_classes': num_classes}, RESUME_PATH)\n",
                "        print(\"  üíæ Saved\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Done! Best loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Results & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "# Loss plot\n",
                "axes[0].plot(history['train'], label='Train')\n",
                "axes[0].plot(history['val'], label='Val')\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
                "axes[0].set_title('Training Loss')\n",
                "\n",
                "# Accuracy plot\n",
                "if history['top1']:\n",
                "    x = list(range(CONFIG['accuracy_check_interval'], len(history['top1'])*CONFIG['accuracy_check_interval']+1, CONFIG['accuracy_check_interval']))\n",
                "    axes[1].plot(x, history['top1'], 'o-', label='Top-1')\n",
                "    axes[1].plot(x, history['top5'], 's-', label='Top-5')\n",
                "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy %'); axes[1].legend()\n",
                "    axes[1].set_title('Identification Accuracy')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "for f in ['best_model.pth', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "        print(f\"‚úì Saved {f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Build References & Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building final reference embeddings...\")\n",
                "ckpt = torch.load(RESUME_PATH)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "test_transform = get_val_transforms()\n",
                "reference_embeddings = []\n",
                "reference_names = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            emb = model(test_transform(image=img)['image'].unsqueeze(0).to(device))\n",
                "            reference_embeddings.append(emb.cpu())\n",
                "            reference_names.append(img_path.stem)\n",
                "        except: pass\n",
                "\n",
                "reference_embeddings = torch.cat(reference_embeddings, dim=0)\n",
                "print(f\"‚úì {len(reference_embeddings):,} embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final accuracy test\n",
                "final_acc = accuracy_monitor.compute_accuracy()\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"FINAL ACCURACY\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Top-1:  {final_acc['top1']:.1f}%\")\n",
                "print(f\"Top-5:  {final_acc['top5']:.1f}%\")\n",
                "print(f\"Top-10: {final_acc['top10']:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load card metadata for identifier\n",
                "import imagehash\n",
                "\n",
                "with open(CARD_JSON, 'r') as f:\n",
                "    all_cards = json.load(f)\n",
                "\n",
                "card_lookup = {c['printing_unique_id']: c for c in all_cards}\n",
                "name_to_printings = {}\n",
                "for card in all_cards:\n",
                "    name = card.get('name', '')\n",
                "    if name not in name_to_printings:\n",
                "        name_to_printings[name] = []\n",
                "    name_to_printings[name].append({\n",
                "        'printing_id': card['printing_unique_id'],\n",
                "        'set_id': card.get('set_id', ''),\n",
                "        'foiling': card.get('foiling', ''),\n",
                "        'phash': card.get('image_phash', '')[:64]\n",
                "    })\n",
                "\n",
                "print(f\"‚úì Loaded {len(all_cards):,} cards, {len(name_to_printings):,} unique names\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TwoStageIdentifier:\n",
                "    def __init__(self, model, ref_embeddings, ref_names, card_lookup, name_to_printings, device):\n",
                "        self.model = model\n",
                "        self.ref_embeddings = ref_embeddings\n",
                "        self.ref_names = ref_names\n",
                "        self.card_lookup = card_lookup\n",
                "        self.name_to_printings = name_to_printings\n",
                "        self.device = device\n",
                "        self.transform = get_val_transforms()\n",
                "    \n",
                "    def identify(self, image_input):\n",
                "        if isinstance(image_input, str):\n",
                "            pil_img = Image.open(image_input).convert('RGB')\n",
                "        else:\n",
                "            pil_img = Image.fromarray(image_input) if isinstance(image_input, np.ndarray) else image_input\n",
                "        \n",
                "        pil_img = ImageOps.autocontrast(pil_img, cutoff=1)\n",
                "        img_array = np.array(pil_img)\n",
                "        \n",
                "        # Stage 1: CNN\n",
                "        with torch.no_grad():\n",
                "            tensor = self.transform(image=img_array)['image'].unsqueeze(0).to(self.device)\n",
                "            query_emb = self.model(tensor).cpu()\n",
                "        \n",
                "        sims = F.cosine_similarity(query_emb, self.ref_embeddings)\n",
                "        top_idx = sims.argsort(descending=True)[:5]\n",
                "        \n",
                "        top_id = self.ref_names[top_idx[0]]\n",
                "        card_info = self.card_lookup.get(top_id, {})\n",
                "        card_name = card_info.get('name', 'Unknown')\n",
                "        \n",
                "        result = {'name': card_name, 'cnn_confidence': sims[top_idx[0]].item()}\n",
                "        \n",
                "        # Stage 2: pHash\n",
                "        query_phash = imagehash.phash(pil_img, hash_size=16)\n",
                "        printings = self.name_to_printings.get(card_name, [])\n",
                "        \n",
                "        if printings:\n",
                "            best = min(printings, key=lambda p: \n",
                "                       (imagehash.hex_to_hash(p['phash']) - query_phash) if p['phash'] else 999)\n",
                "            result.update({'printing_id': best['printing_id'], 'set_id': best['set_id'], \n",
                "                          'foiling': best['foiling']})\n",
                "        \n",
                "        return result\n",
                "\n",
                "identifier = TwoStageIdentifier(model, reference_embeddings, reference_names, \n",
                "                                 card_lookup, name_to_printings, device)\n",
                "print('‚úì TwoStageIdentifier ready!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print('Upload card images to test...')\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    result = identifier.identify(filename)\n",
                "    print(f'\\nüé¥ {result[\"name\"]}')\n",
                "    print(f'   CNN: {result[\"cnn_confidence\"]*100:.1f}%')\n",
                "    print(f'   Printing: {result.get(\"set_id\", \"?\")} ({result.get(\"foiling\", \"?\")})')\n",
                "    \n",
                "    plt.figure(figsize=(5,7))\n",
                "    plt.imshow(Image.open(filename))\n",
                "    plt.title(f'{result[\"name\"]} ({result[\"cnn_confidence\"]*100:.0f}%)')\n",
                "    plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**Curriculum Learning Phases:**\n",
                "- Phase 1 (1-10): Light augmentation\n",
                "- Phase 2 (11-20): Medium augmentation\n",
                "- Phase 3 (21+): Heavy + backgrounds\n",
                "\n",
                "**Live Monitoring:** Top-1/5/10 accuracy every 5 epochs"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}