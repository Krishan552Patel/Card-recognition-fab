{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition Training V3\n",
                "\n",
                "**Mobile-Ready with Synthetic Backgrounds + Two-Stage Identifier**\n",
                "\n",
                "- Synthetic background augmentation (solid colors, gradients)\n",
                "- Enhanced sim-to-real transforms for phone cameras\n",
                "- Two-stage: CNN‚ÜíName, pHash‚ÜíExact Printing\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q timm albumentations opencv-python-headless tqdm imagehash"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Extract Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, zipfile, json\n",
                "from pathlib import Path\n",
                "from PIL import Image, ImageOps\n",
                "from tqdm.notebook import tqdm\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "import cv2\n",
                "import random\n",
                "\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "CARD_JSON = '/content/drive/MyDrive/CardData/card-flattened-with-phash.json'\n",
                "\n",
                "for d in [CHECKPOINT_DIR, DRIVE_OUTPUT]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.extracted\"):\n",
                "    print(f\"‚úì Already extracted\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(\"Extracting...\")\n",
                "    !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
                "        z.extractall(IMAGE_DIR)\n",
                "    Path(f\"{IMAGE_DIR}/.extracted\").touch()\n",
                "    print(f\"‚úì Done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate images\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.validated\"):\n",
                "    print(\"‚úì Already validated\")\n",
                "else:\n",
                "    print(\"Validating...\")\n",
                "    corrupted = []\n",
                "    for p in tqdm(list(Path(IMAGE_DIR).glob('*'))):\n",
                "        if p.suffix.lower() in ['.jpg','.jpeg','.png','.webp']:\n",
                "            try:\n",
                "                with Image.open(p) as img: img.verify()\n",
                "                with Image.open(p) as img: img.load()\n",
                "            except:\n",
                "                corrupted.append(p.name)\n",
                "                p.unlink()\n",
                "    Path(f\"{IMAGE_DIR}/.validated\").touch()\n",
                "    print(f\"‚úì Removed {len(corrupted)} corrupted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Synthetic Background Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SyntheticBackground:\n",
                "    \"\"\"Generate synthetic colored backgrounds and composite cards onto them.\"\"\"\n",
                "    \n",
                "    def __init__(self, output_size=(480, 640)):\n",
                "        self.output_size = output_size  # (height, width)\n",
                "    \n",
                "    def solid_color(self):\n",
                "        color = tuple(random.randint(0, 255) for _ in range(3))\n",
                "        return np.full((*self.output_size, 3), color, dtype=np.uint8)\n",
                "    \n",
                "    def gradient(self):\n",
                "        c1 = np.array([random.randint(0, 255) for _ in range(3)])\n",
                "        c2 = np.array([random.randint(0, 255) for _ in range(3)])\n",
                "        arr = np.zeros((*self.output_size, 3), dtype=np.uint8)\n",
                "        for i in range(self.output_size[0]):\n",
                "            t = i / self.output_size[0]\n",
                "            arr[i] = (c1 * (1 - t) + c2 * t).astype(np.uint8)\n",
                "        return arr\n",
                "    \n",
                "    def noise_pattern(self):\n",
                "        base = np.random.randint(50, 200, 3)\n",
                "        noise = np.random.randint(-30, 30, (*self.output_size, 3))\n",
                "        return np.clip(base + noise, 0, 255).astype(np.uint8)\n",
                "    \n",
                "    def get_random_bg(self):\n",
                "        bg_type = random.choice(['solid', 'solid', 'gradient', 'noise'])\n",
                "        if bg_type == 'solid': return self.solid_color()\n",
                "        elif bg_type == 'gradient': return self.gradient()\n",
                "        else: return self.noise_pattern()\n",
                "    \n",
                "    def composite(self, card_img, apply_perspective=True):\n",
                "        \"\"\"\n",
                "        Composite card onto random background.\n",
                "        card_img: numpy array (H, W, 3)\n",
                "        Returns: numpy array (output_size[0], output_size[1], 3)\n",
                "        \"\"\"\n",
                "        bg = self.get_random_bg()\n",
                "        h, w = card_img.shape[:2]\n",
                "        \n",
                "        # Random scale (40-80% of output height)\n",
                "        scale = random.uniform(0.4, 0.8)\n",
                "        new_h = int(self.output_size[0] * scale)\n",
                "        new_w = int(new_h * w / h)\n",
                "        \n",
                "        # Resize card\n",
                "        card_resized = cv2.resize(card_img, (new_w, new_h))\n",
                "        \n",
                "        # Apply perspective warp\n",
                "        if apply_perspective and random.random() > 0.3:\n",
                "            pts1 = np.float32([[0, 0], [new_w, 0], [0, new_h], [new_w, new_h]])\n",
                "            offset = int(new_w * 0.1)\n",
                "            pts2 = np.float32([\n",
                "                [random.randint(-offset, offset), random.randint(-offset, offset)],\n",
                "                [new_w + random.randint(-offset, offset), random.randint(-offset, offset)],\n",
                "                [random.randint(-offset, offset), new_h + random.randint(-offset, offset)],\n",
                "                [new_w + random.randint(-offset, offset), new_h + random.randint(-offset, offset)]\n",
                "            ])\n",
                "            M = cv2.getPerspectiveTransform(pts1, pts2)\n",
                "            card_resized = cv2.warpPerspective(card_resized, M, (new_w, new_h), \n",
                "                                                borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
                "        \n",
                "        # Random position\n",
                "        max_y = max(0, self.output_size[0] - new_h)\n",
                "        max_x = max(0, self.output_size[1] - new_w)\n",
                "        y = random.randint(0, max_y) if max_y > 0 else 0\n",
                "        x = random.randint(0, max_x) if max_x > 0 else 0\n",
                "        \n",
                "        # Paste (simple blend where card has content)\n",
                "        y_end = min(y + new_h, self.output_size[0])\n",
                "        x_end = min(x + new_w, self.output_size[1])\n",
                "        card_crop = card_resized[:y_end-y, :x_end-x]\n",
                "        \n",
                "        # Create mask (non-black pixels)\n",
                "        mask = (card_crop.sum(axis=2) > 30).astype(np.float32)[:,:,np.newaxis]\n",
                "        bg[y:y_end, x:x_end] = (card_crop * mask + bg[y:y_end, x:x_end] * (1 - mask)).astype(np.uint8)\n",
                "        \n",
                "        return bg\n",
                "\n",
                "synth_bg = SyntheticBackground()\n",
                "print(\"‚úì SyntheticBackground ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "        self.eps = eps\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return F.adaptive_avg_pool2d(x.clamp(min=self.eps).pow(self.p), 1).pow(1./self.p).view(x.size(0), -1)\n",
                "\n",
                "class ColorHistogramBranch(nn.Module):\n",
                "    def __init__(self, bins=32, output_dim=64):\n",
                "        super().__init__()\n",
                "        self.bins = bins\n",
                "        self.fc = nn.Sequential(nn.Linear(bins*3, 128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128, output_dim))\n",
                "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
                "        self.register_buffer('std', torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x_denorm = ((x * self.std + self.mean) * 255).clamp(0, 255)\n",
                "        hists = []\n",
                "        for i in range(x.shape[0]):\n",
                "            img = x_denorm[i].permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
                "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
                "            h = np.histogram(hsv[:,:,0], bins=self.bins, range=(0,180))[0]\n",
                "            s = np.histogram(hsv[:,:,1], bins=self.bins, range=(0,256))[0]\n",
                "            v = np.histogram(hsv[:,:,2], bins=self.bins, range=(0,256))[0]\n",
                "            hist = np.concatenate([h,s,v]).astype(np.float32)\n",
                "            hists.append(hist / (hist.sum() + 1e-8))\n",
                "        return self.fc(torch.tensor(np.stack(hists), device=x.device, dtype=torch.float32))\n",
                "\n",
                "class CardEmbeddingNetV3(nn.Module):\n",
                "    def __init__(self, embedding_dim=512, color_dim=64, pretrained=True):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=pretrained, num_classes=0, global_pool='')\n",
                "        with torch.no_grad():\n",
                "            self.num_features = self.backbone(torch.randn(1,3,224,224)).shape[1]\n",
                "        self.gem = GeM()\n",
                "        self.color_branch = ColorHistogramBranch(bins=32, output_dim=color_dim)\n",
                "        self.fc = nn.Linear(self.num_features + color_dim, embedding_dim)\n",
                "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        visual = self.gem(self.backbone(x))\n",
                "        color = self.color_branch(x)\n",
                "        return F.normalize(self.dropout(self.bn(self.fc(torch.cat([visual, color], dim=1)))), p=2, dim=1)\n",
                "\n",
                "print(\"‚úì Model ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Enhanced Sim-to-Real Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "def get_heavy_augmentations(size=224):\n",
                "    \"\"\"Aggressive sim-to-real augmentations for phone camera robustness.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        # Geometric\n",
                "        A.Perspective(scale=(0.02, 0.1), p=0.5),\n",
                "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
                "        A.Affine(scale=(0.9, 1.1), shear=(-5, 5), p=0.3),\n",
                "        # Camera effects\n",
                "        A.OneOf([\n",
                "            A.MotionBlur(blur_limit=7),\n",
                "            A.GaussianBlur(blur_limit=5),\n",
                "            A.MedianBlur(blur_limit=5),\n",
                "        ], p=0.4),\n",
                "        # Lighting\n",
                "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
                "        A.RandomGamma(gamma_limit=(70, 130), p=0.3),\n",
                "        A.RandomShadow(shadow_roi=(0, 0.3, 1, 1), p=0.2),\n",
                "        # Noise\n",
                "        A.OneOf([\n",
                "            A.GaussNoise(var_limit=(10, 50)),\n",
                "            A.ISONoise(intensity=(0.1, 0.5)),\n",
                "        ], p=0.3),\n",
                "        # Compression\n",
                "        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.3),\n",
                "        # Normalize\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "class SimToRealDataset(Dataset):\n",
                "    def __init__(self, image_dir, transform=None, use_backgrounds=True, rotations=[0,90,180,270]):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.use_backgrounds = use_backgrounds\n",
                "        self.rotations = rotations\n",
                "        self.synth_bg = SyntheticBackground() if use_backgrounds else None\n",
                "        \n",
                "        self.images = sorted([f for f in self.image_dir.iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg','.jpeg','.png','.webp']])\n",
                "        self.num_cards = len(self.images)\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        print(f\"Dataset: {self.num_cards} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rotation = self.samples[idx]\n",
                "        try:\n",
                "            img = np.array(Image.open(self.images[img_idx]).convert('RGB'))\n",
                "            \n",
                "            # Apply rotation\n",
                "            if rotation == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rotation == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rotation == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            \n",
                "            # 50% chance to composite onto synthetic background\n",
                "            if self.use_backgrounds and random.random() > 0.5:\n",
                "                img = self.synth_bg.composite(img)\n",
                "            \n",
                "            if self.transform:\n",
                "                img = self.transform(image=img)['image']\n",
                "            return img, img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples)-1))\n",
                "    \n",
                "    def get_num_classes(self): return self.num_cards\n",
                "\n",
                "print(\"‚úì SimToRealDataset ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Config & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, embedding_dim, scale=64.0, margin=0.5):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, embeddings, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cosine = F.linear(embeddings, W)\n",
                "        one_hot = torch.zeros_like(cosine).scatter_(1, labels.view(-1,1), 1.0)\n",
                "        return F.cross_entropy((cosine - one_hot * self.margin) * self.scale, labels)\n",
                "\n",
                "CONFIG = {\n",
                "    'epochs': 100,\n",
                "    'batch_size': 64,\n",
                "    'learning_rate': 3e-4,\n",
                "    'weight_decay': 5e-4,\n",
                "    'embedding_dim': 512,\n",
                "    'patience': 10,\n",
                "    'unfreeze_epoch': 15,\n",
                "}\n",
                "print(\"‚úì Config:\", CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_dataloaders(image_dir, batch_size=64, val_split=0.15):\n",
                "    train_ds = SimToRealDataset(image_dir, get_heavy_augmentations(), use_backgrounds=True)\n",
                "    val_ds = SimToRealDataset(image_dir, get_val_transforms(), use_backgrounds=False, rotations=[0])\n",
                "    \n",
                "    indices = np.random.permutation(train_ds.num_cards)\n",
                "    split = int((1 - val_split) * train_ds.num_cards)\n",
                "    train_idx, val_idx = set(indices[:split]), set(indices[split:])\n",
                "    \n",
                "    train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "    val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "    \n",
                "    train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                              batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
                "    val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                            batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "    print(f\"Train: {len(train_samples):,} | Val: {len(val_samples):,}\")\n",
                "    return train_loader, val_loader, train_ds.get_num_classes(), train_ds\n",
                "\n",
                "train_loader, val_loader, num_classes, train_ds = create_dataloaders(IMAGE_DIR, CONFIG['batch_size'])\n",
                "print(f\"‚úì Classes: {num_classes:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = CardEmbeddingNetV3(embedding_dim=CONFIG['embedding_dim']).to(device)\n",
                "\n",
                "for p in model.backbone.parameters(): p.requires_grad = False\n",
                "print(\"‚úì Backbone frozen\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['embedding_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                              lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "print(\"‚úì Ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': []}\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    if epoch == CONFIG['unfreeze_epoch']:\n",
                "        print(\"\\nüîì Unfreezing backbone...\")\n",
                "        for p in model.backbone.parameters(): p.requires_grad = True\n",
                "        optimizer = torch.optim.AdamW([\n",
                "            {'params': model.backbone.parameters(), 'lr': CONFIG['learning_rate']/10},\n",
                "            {'params': model.gem.parameters()},\n",
                "            {'params': model.color_branch.parameters()},\n",
                "            {'params': model.fc.parameters()},\n",
                "            {'params': model.bn.parameters()},\n",
                "        ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs']-epoch)\n",
                "    \n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "                    'val_loss': val_loss, 'num_classes': num_classes, 'config': CONFIG}, RESUME_PATH)\n",
                "        print(\"  üíæ Saved\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(\"\\n‚ö†Ô∏è Early stopping!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Done! Best: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Build Reference Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if history['train']:\n",
                "    plt.figure(figsize=(10,4))\n",
                "    plt.plot(history['train'], label='Train')\n",
                "    plt.plot(history['val'], label='Val')\n",
                "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
                "    plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "for f in ['best_model.pth', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "        print(f\"‚úì Saved {f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building reference embeddings...\")\n",
                "ckpt = torch.load(RESUME_PATH)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "test_transform = get_val_transforms()\n",
                "reference_embeddings = []\n",
                "reference_names = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            emb = model(test_transform(image=img)['image'].unsqueeze(0).to(device))\n",
                "            reference_embeddings.append(emb.cpu())\n",
                "            reference_names.append(img_path.stem)\n",
                "        except: pass\n",
                "\n",
                "reference_embeddings = torch.cat(reference_embeddings, dim=0)\n",
                "print(f\"‚úì {len(reference_embeddings):,} embeddings\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Two-Stage Identifier (CNN + pHash)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import imagehash\n",
                "\n",
                "# Load card metadata\n",
                "with open(CARD_JSON, 'r') as f:\n",
                "    all_cards = json.load(f)\n",
                "\n",
                "card_lookup = {c['printing_unique_id']: c for c in all_cards}\n",
                "print(f'‚úì Loaded {len(all_cards):,} cards')\n",
                "\n",
                "# Build name -> printings lookup\n",
                "name_to_printings = {}\n",
                "for card in all_cards:\n",
                "    name = card.get('name', '')\n",
                "    if name not in name_to_printings:\n",
                "        name_to_printings[name] = []\n",
                "    name_to_printings[name].append({\n",
                "        'printing_id': card['printing_unique_id'],\n",
                "        'set_id': card.get('set_id', ''),\n",
                "        'edition': card.get('edition', ''),\n",
                "        'foiling': card.get('foiling', ''),\n",
                "        'card_id': card.get('id', ''),\n",
                "        'phash': card.get('image_phash', '')[:64]\n",
                "    })\n",
                "\n",
                "print(f'‚úì Built name->printings for {len(name_to_printings):,} unique card names')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TwoStageIdentifier:\n",
                "    \"\"\"\n",
                "    Stage 1: CNN identifies card NAME (robust to editions)\n",
                "    Stage 2: pHash identifies exact PRINTING (specific edition/foil)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model, ref_embeddings, ref_names, card_lookup, name_to_printings, device):\n",
                "        self.model = model\n",
                "        self.ref_embeddings = ref_embeddings\n",
                "        self.ref_names = ref_names\n",
                "        self.card_lookup = card_lookup\n",
                "        self.name_to_printings = name_to_printings\n",
                "        self.device = device\n",
                "        self.transform = get_val_transforms()\n",
                "    \n",
                "    def identify(self, image_input, top_k=5):\n",
                "        # Load image\n",
                "        if isinstance(image_input, str):\n",
                "            pil_img = Image.open(image_input).convert('RGB')\n",
                "        elif isinstance(image_input, np.ndarray):\n",
                "            pil_img = Image.fromarray(image_input)\n",
                "        else:\n",
                "            pil_img = image_input.convert('RGB')\n",
                "        \n",
                "        pil_img = ImageOps.autocontrast(pil_img, cutoff=1)\n",
                "        img_array = np.array(pil_img)\n",
                "        \n",
                "        # STAGE 1: CNN - Find card candidates\n",
                "        with torch.no_grad():\n",
                "            tensor = self.transform(image=img_array)['image'].unsqueeze(0).to(self.device)\n",
                "            query_emb = self.model(tensor).cpu()\n",
                "        \n",
                "        sims = F.cosine_similarity(query_emb, self.ref_embeddings)\n",
                "        top_indices = sims.argsort(descending=True)[:top_k]\n",
                "        \n",
                "        top_id = self.ref_names[top_indices[0]]\n",
                "        card_info = self.card_lookup.get(top_id, {})\n",
                "        card_name = card_info.get('name', 'Unknown')\n",
                "        \n",
                "        result = {\n",
                "            'name': card_name,\n",
                "            'cnn_confidence': sims[top_indices[0]].item(),\n",
                "            'cnn_printing_id': top_id,\n",
                "            'cnn_top_matches': [\n",
                "                {'id': self.ref_names[i], 'score': sims[i].item(), \n",
                "                 'name': self.card_lookup.get(self.ref_names[i], {}).get('name', '?')}\n",
                "                for i in top_indices\n",
                "            ]\n",
                "        }\n",
                "        \n",
                "        # STAGE 2: pHash - Find exact printing\n",
                "        query_phash = imagehash.phash(pil_img, hash_size=16)\n",
                "        printings = self.name_to_printings.get(card_name, [])\n",
                "        \n",
                "        if printings:\n",
                "            best_match = None\n",
                "            best_distance = 999\n",
                "            \n",
                "            for p in printings:\n",
                "                if p['phash']:\n",
                "                    try:\n",
                "                        ref_hash = imagehash.hex_to_hash(p['phash'])\n",
                "                        distance = query_phash - ref_hash\n",
                "                        if distance < best_distance:\n",
                "                            best_distance = distance\n",
                "                            best_match = p\n",
                "                    except: pass\n",
                "            \n",
                "            if best_match:\n",
                "                result['printing_id'] = best_match['printing_id']\n",
                "                result['set_id'] = best_match['set_id']\n",
                "                result['card_id'] = best_match['card_id']\n",
                "                result['edition'] = best_match['edition']\n",
                "                result['foiling'] = best_match['foiling']\n",
                "                result['phash_distance'] = best_distance\n",
                "                result['total_printings'] = len(printings)\n",
                "        \n",
                "        return result\n",
                "\n",
                "identifier = TwoStageIdentifier(model, reference_embeddings, reference_names, \n",
                "                                 card_lookup, name_to_printings, device)\n",
                "print('‚úì TwoStageIdentifier ready!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Test with Uploaded Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print('Upload card images to test...')\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    print(f'\\n{\"=\"*60}')\n",
                "    result = identifier.identify(filename)\n",
                "    \n",
                "    print(f'üé¥ {result[\"name\"]}')\n",
                "    print(f'   CNN Confidence: {result[\"cnn_confidence\"]*100:.1f}%')\n",
                "    print(f'   Printing: {result.get(\"set_id\", \"?\")} {result.get(\"card_id\", \"?\")} ({result.get(\"foiling\", \"?\")})')\n",
                "    print(f'   pHash Distance: {result.get(\"phash_distance\", \"N/A\")} (lower=better)')\n",
                "    print(f'   Total printings of this card: {result.get(\"total_printings\", 1)}')\n",
                "    \n",
                "    plt.figure(figsize=(6,8))\n",
                "    plt.imshow(Image.open(filename))\n",
                "    plt.title(f'{result[\"name\"]}\\n{result.get(\"set_id\",\"\")} {result.get(\"card_id\",\"\")}')\n",
                "    plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def quick_id(img_path):\n",
                "    r = identifier.identify(img_path)\n",
                "    print(f'üé¥ {r[\"name\"]} | {r.get(\"set_id\",\"\")} {r.get(\"card_id\",\"\")} | {r[\"cnn_confidence\"]*100:.0f}%')\n",
                "    return r\n",
                "\n",
                "print('‚úì quick_id() ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Done!\n",
                "\n",
                "**Features:**\n",
                "- Synthetic background augmentation\n",
                "- Heavy sim-to-real transforms\n",
                "- Two-stage: CNN‚ÜíName, pHash‚ÜíPrinting\n",
                "\n",
                "**Model:** `MyDrive/CardRecognition_Models/best_model.pth`"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}