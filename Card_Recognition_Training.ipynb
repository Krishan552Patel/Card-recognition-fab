{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¥ Card Recognition V6 - Robust to Real Images\n",
                "\n",
                "**Heavy augmentation from start, frozen backbone**\n",
                "\n",
                "- Aggressive sim-to-real augmentation from epoch 1\n",
                "- Backbone stays frozen (prevents overfitting)\n",
                "- Works with real camera images without preprocessing\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q timm albumentations==1.3.1 opencv-python-headless tqdm imagehash"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, zipfile, json, random\n",
                "from pathlib import Path\n",
                "from PIL import Image, ImageOps\n",
                "from tqdm.notebook import tqdm\n",
                "from collections import Counter\n",
                "import numpy as np\n",
                "import cv2\n",
                "\n",
                "ZIP_PATH = \"/content/drive/MyDrive/CardData/card_images.zip\"\n",
                "IMAGE_DIR = \"/content/card_images\"\n",
                "CHECKPOINT_DIR = '/content/checkpoints'\n",
                "DRIVE_OUTPUT = '/content/drive/MyDrive/CardRecognition_Models'\n",
                "CARD_JSON = '/content/drive/MyDrive/CardData/card-flattened-with-phash.json'\n",
                "\n",
                "for d in [CHECKPOINT_DIR, DRIVE_OUTPUT]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "if os.path.exists(f\"{IMAGE_DIR}/.extracted\"):\n",
                "    print(f\"‚úì Already extracted\")\n",
                "elif os.path.exists(ZIP_PATH):\n",
                "    print(\"Extracting...\")\n",
                "    !rm -rf {IMAGE_DIR}\n",
                "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
                "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
                "        z.extractall(IMAGE_DIR)\n",
                "    Path(f\"{IMAGE_DIR}/.extracted\").touch()\n",
                "    print(f\"‚úì Done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(f\"{IMAGE_DIR}/.validated\"):\n",
                "    print(\"‚úì Already validated\")\n",
                "else:\n",
                "    print(\"Validating...\")\n",
                "    corrupted = []\n",
                "    for p in tqdm(list(Path(IMAGE_DIR).glob('*'))):\n",
                "        if p.suffix.lower() in ['.jpg','.jpeg','.png','.webp']:\n",
                "            try:\n",
                "                with Image.open(p) as img: img.verify()\n",
                "                with Image.open(p) as img: img.load()\n",
                "            except:\n",
                "                corrupted.append(p.name)\n",
                "                p.unlink()\n",
                "    Path(f\"{IMAGE_DIR}/.validated\").touch()\n",
                "    print(f\"‚úì Removed {len(corrupted)} corrupted\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(CARD_JSON, 'r') as f:\n",
                "    all_cards = json.load(f)\n",
                "\n",
                "card_lookup = {c['printing_unique_id']: c for c in all_cards}\n",
                "\n",
                "def get_card_name(printing_id):\n",
                "    card = card_lookup.get(printing_id, {})\n",
                "    name = card.get('name', printing_id[:20])\n",
                "    foil = card.get('foiling', '')\n",
                "    if foil and foil != 'S':\n",
                "        return f\"{name} ({foil})\"\n",
                "    return name\n",
                "\n",
                "print(f\"‚úì Loaded {len(all_cards):,} cards\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ AGGRESSIVE Augmentation (Real-World Simulation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "\n",
                "def get_aggressive_augmentation(size=224):\n",
                "    \"\"\"\n",
                "    Simulate ALL real-world conditions:\n",
                "    - Phone camera blur, noise, compression\n",
                "    - Various lighting (bright, dark, shadows)\n",
                "    - Perspective (card held at angles)\n",
                "    - Color shifts (different light temperatures)\n",
                "    \"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        \n",
                "        # === GEOMETRIC (camera angle, hand shake) ===\n",
                "        A.Perspective(scale=(0.02, 0.12), p=0.6),\n",
                "        A.Rotate(limit=20, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.6),\n",
                "        A.Affine(scale=(0.85, 1.15), shear=(-8, 8), p=0.4),\n",
                "        \n",
                "        # === BLUR (camera focus, motion) ===\n",
                "        A.OneOf([\n",
                "            A.MotionBlur(blur_limit=(3, 9), p=1.0),\n",
                "            A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
                "            A.MedianBlur(blur_limit=5, p=1.0),\n",
                "        ], p=0.5),\n",
                "        \n",
                "        # === LIGHTING (indoor, outdoor, flash) ===\n",
                "        A.OneOf([\n",
                "            A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
                "            A.RandomGamma(gamma_limit=(60, 140), p=1.0),\n",
                "            A.CLAHE(clip_limit=4.0, p=1.0),\n",
                "        ], p=0.7),\n",
                "        \n",
                "        # === SHADOWS (hand shadow, uneven lighting) ===\n",
                "        A.RandomShadow(shadow_roi=(0, 0.3, 1, 1), num_shadows_lower=1, \n",
                "                       num_shadows_upper=2, shadow_dimension=4, p=0.3),\n",
                "        \n",
                "        # === COLOR (different light sources) ===\n",
                "        A.OneOf([\n",
                "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=20, p=1.0),\n",
                "            A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=1.0),\n",
                "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
                "        ], p=0.5),\n",
                "        \n",
                "        # === NOISE (phone camera sensor) ===\n",
                "        A.OneOf([\n",
                "            A.GaussNoise(p=1.0),\n",
                "            A.ISONoise(intensity=(0.1, 0.5), p=1.0),\n",
                "            A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
                "        ], p=0.4),\n",
                "        \n",
                "        # === COMPRESSION (WhatsApp, social media) ===\n",
                "        A.ImageCompression(quality_lower=50, quality_upper=95, p=0.4),\n",
                "        \n",
                "        # === NORMALIZE ===\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(size=224):\n",
                "    \"\"\"Clean validation - no augmentation.\"\"\"\n",
                "    return A.Compose([\n",
                "        A.Resize(size, size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2()\n",
                "    ])\n",
                "\n",
                "print(\"‚úì Aggressive augmentation ready\")\n",
                "print(\"   Simulating: blur, noise, shadows, perspective, lighting, compression\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize augmentation examples\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "sample_images = list(Path(IMAGE_DIR).glob('*.png'))[:3]\n",
                "aug = get_aggressive_augmentation()\n",
                "\n",
                "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
                "for row, img_path in enumerate(sample_images):\n",
                "    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "    axes[row, 0].imshow(img)\n",
                "    axes[row, 0].set_title('Original')\n",
                "    axes[row, 0].axis('off')\n",
                "    \n",
                "    for col in range(1, 5):\n",
                "        augmented = aug(image=img)['image']\n",
                "        # Denormalize for display\n",
                "        display = augmented.permute(1, 2, 0).numpy()\n",
                "        display = display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
                "        display = np.clip(display, 0, 1)\n",
                "        axes[row, col].imshow(display)\n",
                "        axes[row, col].set_title(f'Aug {col}')\n",
                "        axes[row, col].axis('off')\n",
                "\n",
                "plt.suptitle('Aggressive Augmentation Examples', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model (Frozen Backbone)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import timm\n",
                "\n",
                "class GeM(nn.Module):\n",
                "    def __init__(self, p=3.0):\n",
                "        super().__init__()\n",
                "        self.p = nn.Parameter(torch.ones(1) * p)\n",
                "    def forward(self, x):\n",
                "        return F.adaptive_avg_pool2d(x.clamp(min=1e-6).pow(self.p), 1).pow(1./self.p).view(x.size(0), -1)\n",
                "\n",
                "class CardNet(nn.Module):\n",
                "    def __init__(self, emb_dim=512):\n",
                "        super().__init__()\n",
                "        self.backbone = timm.create_model('mobilenetv3_large_100', pretrained=True, \n",
                "                                          num_classes=0, global_pool='')\n",
                "        # Freeze backbone permanently\n",
                "        for p in self.backbone.parameters():\n",
                "            p.requires_grad = False\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            self.n_feat = self.backbone(torch.randn(1, 3, 224, 224)).shape[1]\n",
                "        \n",
                "        self.gem = GeM()\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Linear(self.n_feat, 1024),\n",
                "            nn.BatchNorm1d(1024),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(1024, emb_dim),\n",
                "            nn.BatchNorm1d(emb_dim),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        features = self.gem(self.backbone(x))\n",
                "        emb = self.head(features)\n",
                "        return F.normalize(emb, p=2, dim=1)\n",
                "\n",
                "print(\"‚úì CardNet ready (backbone FROZEN)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class CardDataset(Dataset):\n",
                "    def __init__(self, image_dir, transform, rotations=[0, 90, 180, 270]):\n",
                "        self.images = sorted([f for f in Path(image_dir).iterdir() \n",
                "                              if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']])\n",
                "        self.samples = [(i, r) for i in range(len(self.images)) for r in rotations]\n",
                "        self.transform = transform\n",
                "        print(f\"Dataset: {len(self.images)} cards √ó {len(rotations)} rot = {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_idx, rot = self.samples[idx]\n",
                "        try:\n",
                "            img = np.array(Image.open(self.images[img_idx]).convert('RGB'))\n",
                "            if rot == 90: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
                "            elif rot == 180: img = cv2.rotate(img, cv2.ROTATE_180)\n",
                "            elif rot == 270: img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
                "            return self.transform(image=img)['image'], img_idx\n",
                "        except:\n",
                "            return self.__getitem__(random.randint(0, len(self.samples) - 1))\n",
                "    \n",
                "    def get_num_classes(self): return len(self.images)\n",
                "\n",
                "print(\"‚úì CardDataset ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Accuracy Monitor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AccuracyMonitor:\n",
                "    def __init__(self, model, images, device, n_test=100):\n",
                "        self.model = model\n",
                "        self.images = images\n",
                "        self.device = device\n",
                "        self.n_test = n_test\n",
                "        self.transform = get_val_transforms()\n",
                "        self.ref_embeddings = None\n",
                "        self.ref_ids = None\n",
                "        self.confusions = Counter()\n",
                "    \n",
                "    def build_references(self):\n",
                "        self.model.eval()\n",
                "        embeddings, ids = [], []\n",
                "        with torch.no_grad():\n",
                "            for img_path in self.images:\n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    emb = self.model(self.transform(image=img)['image'].unsqueeze(0).to(self.device))\n",
                "                    embeddings.append(emb.cpu())\n",
                "                    ids.append(img_path.stem)\n",
                "                except: pass\n",
                "        self.ref_embeddings = torch.cat(embeddings, dim=0)\n",
                "        self.ref_ids = ids\n",
                "    \n",
                "    def compute_accuracy(self):\n",
                "        if self.ref_embeddings is None:\n",
                "            self.build_references()\n",
                "        \n",
                "        self.model.eval()\n",
                "        self.confusions.clear()\n",
                "        \n",
                "        test_idx = random.sample(range(len(self.images)), min(self.n_test, len(self.images)))\n",
                "        top1, top5, top10 = 0, 0, 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for idx in test_idx:\n",
                "                img_path = self.images[idx]\n",
                "                actual_id = img_path.stem\n",
                "                try:\n",
                "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
                "                    query = self.model(self.transform(image=img)['image'].unsqueeze(0).to(self.device)).cpu()\n",
                "                    sims = F.cosine_similarity(query, self.ref_embeddings)\n",
                "                    top_idx = sims.argsort(descending=True)[:10]\n",
                "                    top_ids = [self.ref_ids[i] for i in top_idx]\n",
                "                    \n",
                "                    if actual_id == top_ids[0]: top1 += 1\n",
                "                    if actual_id in top_ids[:5]: top5 += 1\n",
                "                    if actual_id in top_ids[:10]: top10 += 1\n",
                "                    \n",
                "                    if actual_id != top_ids[0]:\n",
                "                        self.confusions[(get_card_name(actual_id), get_card_name(top_ids[0]))] += 1\n",
                "                except: pass\n",
                "        \n",
                "        n = len(test_idx)\n",
                "        return {'top1': 100*top1/n, 'top5': 100*top5/n, 'top10': 100*top10/n}\n",
                "    \n",
                "    def print_confusions(self, n=5):\n",
                "        top = self.confusions.most_common(n)\n",
                "        if top:\n",
                "            print(f\"\\n   üîÑ TOP {len(top)} CONFUSIONS:\")\n",
                "            for (a, p), c in top:\n",
                "                print(f\"      '{a}' ‚Üí '{p}' ({c}x)\")\n",
                "\n",
                "print(\"‚úì AccuracyMonitor ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CosFaceLoss(nn.Module):\n",
                "    def __init__(self, num_classes, emb_dim, scale=30.0, margin=0.35):\n",
                "        super().__init__()\n",
                "        self.scale, self.margin = scale, margin\n",
                "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, emb_dim))\n",
                "        nn.init.xavier_uniform_(self.weight)\n",
                "    \n",
                "    def forward(self, emb, labels):\n",
                "        W = F.normalize(self.weight, p=2, dim=1)\n",
                "        cos = F.linear(emb, W)\n",
                "        one_hot = torch.zeros_like(cos).scatter_(1, labels.view(-1, 1), 1.0)\n",
                "        return F.cross_entropy((cos - one_hot * self.margin) * self.scale, labels)\n",
                "\n",
                "CONFIG = {\n",
                "    'epochs': 50,\n",
                "    'batch_size': 64,\n",
                "    'lr': 5e-4,\n",
                "    'weight_decay': 1e-4,\n",
                "    'emb_dim': 512,\n",
                "    'patience': 20,  # More patience for augmented data\n",
                "    'check_interval': 5,\n",
                "}\n",
                "print(\"‚úì Config:\", CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets - AGGRESSIVE augmentation for training\n",
                "train_ds = CardDataset(IMAGE_DIR, get_aggressive_augmentation())\n",
                "val_ds = CardDataset(IMAGE_DIR, get_val_transforms(), rotations=[0])\n",
                "\n",
                "# Split\n",
                "indices = np.random.permutation(len(train_ds.images))\n",
                "split = int(0.85 * len(train_ds.images))\n",
                "train_idx, val_idx = set(indices[:split]), set(indices[split:])\n",
                "\n",
                "train_samples = [i for i, (c, _) in enumerate(train_ds.samples) if c in train_idx]\n",
                "val_samples = [i for i, (c, _) in enumerate(val_ds.samples) if c in val_idx]\n",
                "\n",
                "train_loader = DataLoader(torch.utils.data.Subset(train_ds, train_samples),\n",
                "                          batch_size=CONFIG['batch_size'], shuffle=True, \n",
                "                          num_workers=2, pin_memory=True, drop_last=True)\n",
                "val_loader = DataLoader(torch.utils.data.Subset(val_ds, val_samples),\n",
                "                        batch_size=CONFIG['batch_size'], shuffle=False, \n",
                "                        num_workers=2, pin_memory=True)\n",
                "\n",
                "num_classes = train_ds.get_num_classes()\n",
                "print(f\"‚úì Train: {len(train_samples):,} | Val: {len(val_samples):,} | Classes: {num_classes:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = CardNet(emb_dim=CONFIG['emb_dim']).to(device)\n",
                "\n",
                "# Only train the head (backbone is already frozen in model)\n",
                "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
                "print(f\"‚úì Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
                "\n",
                "criterion = CosFaceLoss(num_classes, CONFIG['emb_dim']).to(device)\n",
                "optimizer = torch.optim.AdamW(trainable_params + list(criterion.parameters()),\n",
                "                              lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "accuracy_monitor = AccuracyMonitor(model, train_ds.images, device, n_test=100)\n",
                "\n",
                "print(\"‚úì Ready to train!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "history = {'train': [], 'val': [], 'top1': [], 'top5': []}\n",
                "RESUME_PATH = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"V6: ROBUST TRAINING (Heavy Augmentation from Start)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            loss = criterion(model(images), labels)\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            val_loss += criterion(model(images), labels).item()\n",
                "    val_loss /= len(val_loader)\n",
                "    \n",
                "    scheduler.step()\n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    # Accuracy check\n",
                "    if epoch % CONFIG['check_interval'] == 0:\n",
                "        acc = accuracy_monitor.compute_accuracy()\n",
                "        history['top1'].append(acc['top1'])\n",
                "        history['top5'].append(acc['top5'])\n",
                "        print(f\"\\nEpoch {epoch}: Loss={train_loss:.2f}/{val_loss:.2f} | \"\n",
                "              f\"Top-1: {acc['top1']:.1f}% | Top-5: {acc['top5']:.1f}%\")\n",
                "        accuracy_monitor.print_confusions(n=5)\n",
                "    else:\n",
                "        print(f\"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
                "    \n",
                "    # Save best\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
                "                    'val_loss': val_loss, 'num_classes': num_classes}, RESUME_PATH)\n",
                "        print(\"  üíæ Saved\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= CONFIG['patience']:\n",
                "            print(\"\\n‚ö†Ô∏è Early stop!\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úì Done! Best: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "axes[0].plot(history['train'], label='Train')\n",
                "axes[0].plot(history['val'], label='Val')\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
                "axes[0].set_title('Loss')\n",
                "\n",
                "if history['top1']:\n",
                "    x = list(range(CONFIG['check_interval'], len(history['top1'])*CONFIG['check_interval']+1, CONFIG['check_interval']))\n",
                "    axes[1].plot(x, history['top1'], 'o-', label='Top-1')\n",
                "    axes[1].plot(x, history['top5'], 's-', label='Top-5')\n",
                "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy %'); axes[1].legend()\n",
                "    axes[1].set_title('Accuracy')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CHECKPOINT_DIR}/training.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "for f in ['best_model.pth', 'training.png']:\n",
                "    src = f\"{CHECKPOINT_DIR}/{f}\"\n",
                "    if os.path.exists(src):\n",
                "        shutil.copy(src, DRIVE_OUTPUT)\n",
                "        print(f\"‚úì Saved {f} to Drive\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Test with Real Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building reference embeddings...\")\n",
                "ckpt = torch.load(RESUME_PATH)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "ref_embeddings, ref_names = [], []\n",
                "with torch.no_grad():\n",
                "    for img_path in tqdm(train_ds.images, desc=\"Building refs\"):\n",
                "        try:\n",
                "            img = np.array(Image.open(img_path).convert('RGB'))\n",
                "            emb = model(get_val_transforms()(image=img)['image'].unsqueeze(0).to(device))\n",
                "            ref_embeddings.append(emb.cpu())\n",
                "            ref_names.append(img_path.stem)\n",
                "        except: pass\n",
                "\n",
                "ref_embeddings = torch.cat(ref_embeddings, dim=0)\n",
                "print(f\"‚úì {len(ref_embeddings):,} reference embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def identify_card(img_path, top_k=5):\n",
                "    \"\"\"\n",
                "    Identify a card from ANY image - no preprocessing needed!\n",
                "    Works with real camera photos.\n",
                "    \"\"\"\n",
                "    # Load image AS-IS (no preprocessing)\n",
                "    pil_img = Image.open(img_path).convert('RGB')\n",
                "    img_array = np.array(pil_img)\n",
                "    \n",
                "    # Get embedding\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        tensor = get_val_transforms()(image=img_array)['image'].unsqueeze(0).to(device)\n",
                "        query_emb = model(tensor).cpu()\n",
                "    \n",
                "    # Find matches\n",
                "    sims = F.cosine_similarity(query_emb, ref_embeddings)\n",
                "    top_indices = sims.argsort(descending=True)[:top_k]\n",
                "    \n",
                "    results = []\n",
                "    for idx in top_indices:\n",
                "        printing_id = ref_names[idx]\n",
                "        card = card_lookup.get(printing_id, {})\n",
                "        results.append({\n",
                "            'name': card.get('name', 'Unknown'),\n",
                "            'confidence': sims[idx].item(),\n",
                "            'set': card.get('set_id', ''),\n",
                "            'foil': card.get('foiling', '')\n",
                "        })\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"‚úì identify_card() ready - works with raw camera images!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with uploaded images\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Upload REAL camera images to test:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Testing: {filename}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    results = identify_card(filename, top_k=5)\n",
                "    \n",
                "    print(f\"\\nüé¥ TOP 5 MATCHES:\")\n",
                "    for i, r in enumerate(results):\n",
                "        status = \"‚úì\" if i == 0 else \" \"\n",
                "        print(f\"   {status} {i+1}. {r['name']} ({r['confidence']*100:.1f}%)\")\n",
                "        print(f\"        Set: {r['set']} | Foil: {r['foil']}\")\n",
                "    \n",
                "    plt.figure(figsize=(6, 8))\n",
                "    plt.imshow(Image.open(filename))\n",
                "    plt.title(f\"Top match: {results[0]['name']}\\n({results[0]['confidence']*100:.1f}%)\")\n",
                "    plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ V6 Complete!\n",
                "\n",
                "**Key differences from V5:**\n",
                "- Heavy augmentation from epoch 1 (no phases)\n",
                "- Backbone stays frozen (no unfreezing)\n",
                "- Larger backbone (mobilenetv3_large vs small)\n",
                "- Lower CosFace scale (30 vs 64) for stability\n",
                "\n",
                "**Should work with:**\n",
                "- Phone camera photos\n",
                "- Different lighting\n",
                "- Angled cards\n",
                "- Slight blur\n",
                "\n",
                "**No preprocessing needed at inference!**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}